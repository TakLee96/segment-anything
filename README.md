# CSCI567

## file structure
To evalua all the method, make sure evaluation data and sam model in this structure:
```md
segment-anything
├── sam_vit_h_4b8939.pth
└── datasets/people_poses
    ├── train_images/*.jpg
    ├── train_segementations/*.png
    ├── val_images/*.jpg
    ├── val_segementations/*.png
    ├── train_id.txt
    ├── val_id.txt
    └── val_id_short.txt
```
- [dataset readme](datasets/people_poses/README.md)
- val_id_short.txt include the 20 first image from val set for visualization and comparation of all method
- SAM model download by 
```bash
wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth
```

The evaluation file and linear probe training file structure:
```md
segment-anything
└── notebooks
    ├── eval
    │   ├── xxx_result
    │   │   ├── miou.csv
    │   │   └── pixacc.csv
    │   ├── *.py
    │   ├── *_colab.ipynb
    │   └── *_local_machine.ipynb
    ├── train
    │   ├── *.py
    │   ├── gen_embedding.ipynb
    │   └── gen_embedding.ipynb
    └── vis
        ├── *.py
        ├── *.ipynb
        ├── *.png
        └── *.npy
```
- in eval folder:
  - *_colab.ipynb: small evaluation test on colab 
  - *_local_machine.ipynb: small evaluation test on local mechine 
  - *.py: evaluation for whole val data set on local mechine 
- in train folder:
  - gen_embedding.ipynb: generate image embeding by SAM for speeding up the trainning processe 
  - train_embedding*.ipynb: try different version of linear probe structure setup and test program 
  - train_embedding.py: train final version linear probe method based on image embeddings
- in vis folder:
  - compare_vis.ipynb: compare predicted masks generated by different model for different class in val_id_short.txt  
  - *_local_machine.ipynb: small evaluation test on local mechine 
  - *.py: evaluation for whole val data set on local mechine 
## preparation
 python>=3.8, as well as pytorch>=1.7 and torchvision>=0.8. 

1. Add people_poses data set into datasets [dataset readme](datasets/people_poses/README.md) and [link](https://drive.google.com/drive/folders/1yB5x4H0m3LszjpErRDDXITDxzlFgQEm7?usp=sharing) 

2. Add Sam model to this repo root folder
## train linear probe model
1. generate image embeding by [gen_embedding.ipynb](notebooks/train/gen_embedding.ipynb)
2. use ipynb or py file in the train folder to train the linear probe model
## evaluation
1. use *_local_machine.ipynb to setup different model enviroment, and try on small data amount of data and get some visualization result
2. use *.py file to generate the csv result for each model (the interactive method based on image embedding data)
```bash
cd notebooks/eval
python <model_name>.py
```
3. reulst will show up in notebooks/eval/*result folder


## reference
- [CLIP-SAM](https://github.com/maxi-w/CLIP-SAM)
- [grounded-segment-anything](https://github.com/IDEA-Research/Grounded-Segment-Anything)
- [mmdetection](https://github.com/open-mmlab/mmdetection)
- [CLIPSeg-sam](https://huggingface.co/blog/clipseg-zero-shot)
- [interactive-segmentation](https://github.com/SamsungLabs/ritm_interactive_segmentation)
- [segment anything](https://github.com/SamsungLabs/ritm_interactive_segmentation](https://github.com/facebookresearch/segment-anything)https://github.com/facebookresearch/segment-anything)
---



