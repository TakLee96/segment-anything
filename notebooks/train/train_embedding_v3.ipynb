{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfd1487a-2c32-45f9-bbde-f42f1c98fc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jiaha\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3824455f-3acc-4d3b-aeef-ca78ea9ba710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from segment_anything.modeling import TwoWayTransformer\n",
    "from segment_anything.modeling.common import LayerNorm2d\n",
    "from segment_anything.modeling.mask_decoder import MLP\n",
    "from segment_anything.modeling.prompt_encoder import PositionEmbeddingRandom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff73e710-d541-496a-8a46-e4f12c038681",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "\n",
    "class MaskDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        transformer_dim,\n",
    "        transformer,\n",
    "        num_multimask_outputs,\n",
    "        activation = nn.GELU,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.transformer_dim = transformer_dim\n",
    "        self.transformer = transformer\n",
    "        self.num_multimask_outputs = num_multimask_outputs\n",
    "        self.num_mask_tokens = num_multimask_outputs\n",
    "        self.mask_tokens = nn.Embedding(self.num_mask_tokens, transformer_dim)\n",
    "        self.output_upscaling = nn.Sequential(\n",
    "            nn.ConvTranspose2d(transformer_dim, transformer_dim // 4, kernel_size=2, stride=2),\n",
    "            LayerNorm2d(transformer_dim // 4),\n",
    "            activation(),\n",
    "            nn.ConvTranspose2d(transformer_dim // 4, transformer_dim // 8, kernel_size=2, stride=2),\n",
    "            activation(),\n",
    "        )\n",
    "        self.output_hypernetworks_mlps = nn.ModuleList([\n",
    "            MLP(transformer_dim, transformer_dim, transformer_dim // 8, 3)\n",
    "                for i in range(self.num_mask_tokens)\n",
    "        ])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        image_embeddings,\n",
    "        image_pe,\n",
    "    ):\n",
    "        tokens = self.mask_tokens.weight.unsqueeze(0) # 1, 20, 256\n",
    "        src = image_embeddings\n",
    "        pos_src = image_pe\n",
    "        b, c, h, w = src.shape\n",
    "\n",
    "        # Run the transformer\n",
    "        mask_tokens_out, src = self.transformer(src, pos_src, tokens)\n",
    "\n",
    "        # Upscale mask embeddings and predict masks using the mask tokens\n",
    "        src = src.transpose(1, 2).view(b, c, h, w)\n",
    "        upscaled_embedding = self.output_upscaling(src)\n",
    "        hyper_in_list = []\n",
    "        for i in range(self.num_mask_tokens):\n",
    "            hyper_in_list.append(self.output_hypernetworks_mlps[i](mask_tokens_out[:, i, :]))\n",
    "        hyper_in = torch.stack(hyper_in_list, dim=1)\n",
    "        b, c, h, w = upscaled_embedding.shape\n",
    "        masks = (hyper_in @ upscaled_embedding.view(b, c, h * w)).view(b, -1, h, w)\n",
    "\n",
    "        return masks\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pe_layer = PositionEmbeddingRandom(128)\n",
    "        self.mask_decoder = MaskDecoder(\n",
    "            num_multimask_outputs=20,\n",
    "            transformer=TwoWayTransformer(\n",
    "                depth=2,\n",
    "                embedding_dim=256,\n",
    "                mlp_dim=2048,\n",
    "                num_heads=8,\n",
    "            ),\n",
    "            transformer_dim=256,\n",
    "        )\n",
    "\n",
    "    def forward(self, img_embed):\n",
    "        \"\"\" img_embed: (B, 256, 64, 64)\n",
    "            @returns (B, C, 256, 256) logits\n",
    "        \"\"\"\n",
    "        image_pe = self.pe_layer((64, 64)).unsqueeze(0)\n",
    "        return self.mask_decoder(img_embed, image_pe)\n",
    "\n",
    "model = Decoder()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65f78ec8-9a97-417d-8afe-c02c0f6af61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "class PeoplePosesDataset(Dataset):\n",
    "    def __init__(self, mode=\"train\", img_size=1024):\n",
    "        assert mode in (\"train\", \"val\")\n",
    "        self.mode = mode\n",
    "        self.root = \"../../datasets/people_poses\"\n",
    "        self.image_dir = os.path.join(self.root, f\"{self.mode}_images\")\n",
    "        self.mask_dir = os.path.join(self.root, f\"{self.mode}_segmentations\")\n",
    "        self.embed_dir = os.path.join(self.root, f\"{self.mode}_embeds\")\n",
    "        with open(os.path.join(self.root, f\"{self.mode}_id.txt\"), 'r') as lf:\n",
    "            self.data_list = [ s.strip() for s in lf.readlines() ]\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" embed: (1, 256, 64, 64)\n",
    "            label: (1, 256, 256)\n",
    "        \"\"\"\n",
    "        data = np.load(os.path.join(self.embed_dir, self.data_list[index] + \".npz\"))\n",
    "        embed = data['embed']\n",
    "        embed = torch.as_tensor(embed)\n",
    "        label = data['label']\n",
    "        label[label == 0] = -1 # ignore background\n",
    "        # NOTE(jiahang): uint8 cannot handle -1, other int fails F.interpolate\n",
    "        label = torch.as_tensor(label).to(torch.float32)\n",
    "        label = F.interpolate(label[None, None, ...], (256, 256), mode='nearest')\n",
    "        return embed, label[0][0].to(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a5693b-ee25-43dd-9a68-8f81a8d6177c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0: LR=0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.109055757522583:   7%|███████▋                                                                                                 | 70/952 [01:46<19:01,  1.29s/it]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import SequentialLR, ConstantLR, ExponentialLR\n",
    "\n",
    "dataset = PeoplePosesDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "def loss_fn(logits, labels):\n",
    "    \"\"\" logits/labels: (B, C, 1024, 1024) \"\"\"\n",
    "    return F.cross_entropy(logits, labels, ignore_index=-1, reduction='mean')\n",
    "\n",
    "def train_one_epoch(epoch_index):\n",
    "    losses = []\n",
    "    for data in (pbar := tqdm(dataloader)):\n",
    "        embeds, labels = data\n",
    "        embeds = embeds.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(embeds)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pbar.set_description(f'loss: {loss}')\n",
    "    print('Average Loss: {}'.format(np.mean(losses)))\n",
    "    scheduler.step()\n",
    "\n",
    "os.makedirs('v3', exist_ok=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = SequentialLR(optimizer, schedulers=[\n",
    "    ConstantLR(optimizer, factor=0.1, total_iters=1),\n",
    "    ExponentialLR(optimizer, gamma=0.9)\n",
    "], milestones=[1])\n",
    "\n",
    "for epoch in range(20):\n",
    "    print('EPOCH {}: LR={}'.format(epoch, optimizer.param_groups[0]['lr']))\n",
    "\n",
    "    model.train(True)\n",
    "    train_one_epoch(epoch)\n",
    "\n",
    "    model_path = 'v3/model_{}.pth'.format(epoch)\n",
    "    torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f158a8-4198-479c-a336-a48c5a3713fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1340bae-4b48-443f-a3dc-3f766d7696a7",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ad9c49-77f2-4593-a5b2-0524f46a3aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = PeoplePosesDataset(mode=\"val\")\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd632a38-58d8-4d8c-a1db-a67f359f788d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = val_dataset[0]\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e78a584-f0a3-45c8-b73d-84c7ef8fec75",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(os.path.join(val_dataset.image_dir, val_dataset.data_list[0] + \".jpg\"))\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a82290-085e-4297-9eff-9e4a165f96dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "transform = ResizeLongestSide(1024)\n",
    "\n",
    "def preprocess(x):\n",
    "    h, w = x.shape[-2:]\n",
    "    padh = 1024 - h\n",
    "    padw = 1024 - w\n",
    "    x = F.pad(x, (0, padw, 0, padh))\n",
    "    return x\n",
    "\n",
    "def preprocess_image(image):\n",
    "    image = transform.apply_image(image)\n",
    "    H, W, C = image.shape\n",
    "    image = torch.as_tensor(image)\n",
    "    image = image.permute(2, 0, 1).contiguous()[None, :, :, :]\n",
    "    image = preprocess(image)\n",
    "    return image\n",
    "\n",
    "prep = preprocess_image(image)\n",
    "prep = prep[0].permute(1, 2, 0).numpy()\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(prep)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3880ccef-c84e-446d-8c4e-12924fb6e6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gt_to_anns(mask_gt):\n",
    "    labels = np.unique(mask_gt)\n",
    "    anns = []\n",
    "    for label in labels:\n",
    "        mask = np.all(mask_gt == label, axis=-1)\n",
    "        anns.append({\n",
    "            'area': np.sum(mask),\n",
    "            'segmentation': mask,\n",
    "            'label': label\n",
    "        })\n",
    "    return anns\n",
    "\n",
    "def show_anns(anns):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "\n",
    "    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
    "    img[:,:,3] = 0\n",
    "    for ann in sorted_anns:\n",
    "        m = ann['segmentation']\n",
    "        color_mask = np.concatenate([np.random.random(3), [0.35]])\n",
    "        img[m] = color_mask\n",
    "    ax.imshow(img)\n",
    "\n",
    "label = y[:,:,None]\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(prep)\n",
    "show_anns(gt_to_anns(label))\n",
    "plt.axis('off')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9f389b-e54b-4897-a831-c5f1d02ff97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    logit = model(torch.as_tensor(x[None,...]).to(device))\n",
    "logit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d838aefa-251e-482d-ad42-f65c2619fddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = logit.cpu().numpy().argmax(axis=1)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27d58e3-76c9-409e-acdd-a7a4110d31a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.transpose(pred, [1, 2, 0])\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(prep)\n",
    "show_anns(gt_to_anns(pred))\n",
    "plt.axis('off')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a462423-bf54-4de1-8a7f-6fade5307fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE(jiahang): this evaluation has bug, because it considered padding(-1)\n",
    "(pred == label).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2240334a-82aa-4937-a64a-a70579500d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302e81c5-e46a-4200-b954-6213a63204a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_txt = \"\"\"Background\n",
    "Hat\n",
    "Hair\n",
    "Glove\n",
    "Sunglasses\n",
    "UpperClothes\n",
    "Dress\n",
    "Coat\n",
    "Socks\n",
    "Pants\n",
    "Jumpsuits\n",
    "Scarf\n",
    "Skirt\n",
    "Face\n",
    "Left-arm\n",
    "Right-arm\n",
    "Left-leg\n",
    "Right-leg\n",
    "Left-shoe\n",
    "Right-shoe\"\"\"\n",
    "label_txt = label_txt.split('\\n')\n",
    "label_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1a66ed-0c05-43fc-91c0-291a60831692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE(jiahang): this evaluation has bug, because it considered padding(-1)\n",
    "ious = []\n",
    "pixels = []\n",
    "for i in range(20):\n",
    "    label_i = (label == i)\n",
    "    pred_i = (pred == i)\n",
    "    ious.append((label_i & pred_i).sum() / ((label_i | pred_i).sum() + 1e-6))\n",
    "    pixels.append(label_i.sum())\n",
    "print(np.mean(ious))\n",
    "pd.DataFrame({ 'labels': label_txt, 'ious': ious, 'pixels': pixels })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7e2f4b-0f23-4c21-b1bf-cec276f8964d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
