{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuSVN-W_lSFS"
      },
      "source": [
        "# CLIP + SAM\n",
        "- Environment Setting\n",
        "- CLIP + SAM Evaluation on People Pose\n",
        "- Hyperparameter Finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCLJ7T_wmCl3"
      },
      "source": [
        "## Environment Setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eMudSuPmapC"
      },
      "source": [
        "### Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sSLToQlqmHaX",
        "outputId": "8bd298c5-3300-4aa0-b8d0-b1829454b5ef"
      },
      "outputs": [],
      "source": [
        "# !pip install torch opencv-python Pillow\n",
        "# !pip install git+https://github.com/openai/CLIP.git\n",
        "# !pip install git+https://github.com/facebookresearch/segment-anything.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOJoO73Wmcrz"
      },
      "source": [
        "### Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "n5BT4xS0mIhg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.jit import Error\n",
        "\n",
        "# numpy metrics\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "import cv2\n",
        "from segment_anything import build_sam, SamAutomaticMaskGenerator\n",
        "from PIL import Image, ImageDraw\n",
        "import clip\n",
        "\n",
        "import sys\n",
        "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfsuC046mfRT"
      },
      "source": [
        "### Image Processing, Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jHzj7O61mOHR"
      },
      "outputs": [],
      "source": [
        "# Convert Mask's Boundary Box from XYWH to XYXY format\n",
        "def convert_box_xywh_to_xyxy(box):\n",
        "  x1 = box[0]\n",
        "  y1 = box[1]\n",
        "  x2 = box[0] + box[2]\n",
        "  y2 = box[1] + box[3]\n",
        "  if(box[2]==0 or box[3]==0):\n",
        "    print(box[2],box[3],[x1, y1, x2, y2])\n",
        "  return [x1, y1, x2, y2]\n",
        "\n",
        "# Show Only the Segmented Part in the Given Image\n",
        "def segment_image(image, segmentation_mask):\n",
        "    image_array = np.array(image)\n",
        "    segmented_image_array = np.zeros_like(image_array)\n",
        "    segmented_image_array[segmentation_mask] = image_array[segmentation_mask]\n",
        "    segmented_image = Image.fromarray(segmented_image_array)\n",
        "    black_image = Image.new(\"RGB\", image.size, (0, 0, 0))\n",
        "    transparency_mask = np.zeros_like(segmentation_mask, dtype=np.uint8)\n",
        "    transparency_mask[segmentation_mask] = 255\n",
        "    transparency_mask_image = Image.fromarray(transparency_mask, mode='L')\n",
        "    black_image.paste(segmented_image, mask=transparency_mask_image)\n",
        "    return black_image\n",
        "\n",
        "def gt_to_anns_of_label_mask(mask_gt):\n",
        "  labels = np.unique(mask_gt)\n",
        "  anns = []\n",
        "  for label in labels:\n",
        "    # skip background\n",
        "      if label == 0:\n",
        "          continue\n",
        "      mask = np.all(mask_gt == label, axis=-1)\n",
        "      anns.append({\n",
        "        'area': np.sum(mask),\n",
        "        'segmentation': mask,\n",
        "        'label': label,\n",
        "      })\n",
        "  return anns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9eBAPNUmi-0"
      },
      "source": [
        "### Retrieve Similarity between Image and Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8B9KQuHEmV3k"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def retriev(image_features: list, search_text: str) -> int:\n",
        "    # preprocessed_images = [preprocess(image).to(device) for image in elements]\n",
        "    tokenized_text = clip.tokenize([search_text]).to(device)\n",
        "    # stacked_images = torch.stack(preprocessed_images)\n",
        "    # image_features = model.encode_image(stacked_images)\n",
        "    text_features = model.encode_text(tokenized_text)\n",
        "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "    probs = 100. * image_features @ text_features.T\n",
        "    return probs[:, 0].softmax(dim=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "la4qev_-nA6g"
      },
      "source": [
        "### CUDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Dub6LN3km9J_"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pY1iraBumxJ9"
      },
      "source": [
        "### Load CLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cVvX69Rmysm",
        "outputId": "71641389-5201-4c8b-f466-7fe9f83960cf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████| 338M/338M [00:48<00:00, 7.26MiB/s]\n"
          ]
        }
      ],
      "source": [
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ABw3iAPnGVA"
      },
      "source": [
        "### Load SAM (Default Params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "boLctCM6nFsB"
      },
      "outputs": [],
      "source": [
        "sam_checkpoint = \"../../sam_vit_h_4b8939.pth\"\n",
        "model_type = \"vit_h\"\n",
        "\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "sam = sam.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KnxVEDEittdz"
      },
      "outputs": [],
      "source": [
        "mask_generator_default = SamAutomaticMaskGenerator(sam)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPZYi5mwnPlS"
      },
      "source": [
        "### Load SAM (Best Params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3ea6l6xooMW"
      },
      "source": [
        "### Class Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kEuhjWEToquH"
      },
      "outputs": [],
      "source": [
        "LABELS = [\"Background\",\"Hat\",\"Hair\",\"Glove\",\n",
        "        \"Sunglasses\",\"UpperClothes\",\"Dress\",\"Coat\",\"Socks\",\"Pants\",\n",
        "        \"Jumpsuits\",\"Scarf\",\"Skirt\",\"Face\",\"Left-arm\",\"Right-arm\",\"Left-leg\",\"Right-leg\",\"Left-shoe\",\"Right-shoe\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZt4xM4Fl2oG"
      },
      "source": [
        "## CLIP + SAM Evaluation on People Pose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "k7fMnYuto3nQ"
      },
      "outputs": [],
      "source": [
        "def compute_pix_acc(predicted, target):\n",
        "    assert predicted.shape == target.shape\n",
        "    assert len(predicted.shape) == 2\n",
        "    return (predicted == target).mean()\n",
        "\n",
        "def compute_IOU(predicted, target):\n",
        "    assert predicted.shape == target.shape\n",
        "    assert len(predicted.shape) == 2\n",
        "    intersection = np.logical_and(target, predicted).sum()\n",
        "    union = np.logical_or(target, predicted).sum()\n",
        "    assert union > 0\n",
        "    return intersection / union\n",
        "def compute_metric(name, masks, label):\n",
        "    \"\"\" name: data_id\n",
        "        mask: { label_id: numpy.ndarray(shape=(H, W)) }\n",
        "        label: np.ndarray(shape=(H, W)) --> numbers from 0 to 19\n",
        "    \"\"\"\n",
        "    pix_acc_metric = { \"name\": name }\n",
        "    iou_metric = { \"name\": name }\n",
        "    empty = np.zeros_like(label)\n",
        "    for i, label_name in enumerate(LABELS):\n",
        "        mask_i = masks.get(i, empty)\n",
        "        label_i = (label == i)\n",
        "        if label_i.sum() == 0:\n",
        "            # pandas dataframe automatically skips nan\n",
        "            # when computing .count() and .mean()\n",
        "            iou_metric[label_name] = np.nan\n",
        "            pix_acc_metric[label_name] = np.nan\n",
        "        else:\n",
        "            iou_metric[label_name] = compute_IOU(mask_i, label_i)\n",
        "            pix_acc_metric[label_name] = compute_pix_acc(mask_i, label_i)\n",
        "\n",
        "    return iou_metric, pix_acc_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "pg6iwlxWo8uZ"
      },
      "outputs": [],
      "source": [
        "def evaluate(sam_generator, data_cnt=100):\n",
        "  root = \"../../datasets/people_poses/\"\n",
        "  prompt = \"The object of \"\n",
        "  with open(os.path.join(root, f\"val_id.txt\"), 'r') as lf:\n",
        "      data_list = [ s.strip() for s in lf.readlines() ]\n",
        "  if data_cnt > len(data_list):\n",
        "    evaluate_data = data_list\n",
        "  else:\n",
        "    evaluate_data = data_list[:data_cnt]\n",
        "  try:\n",
        "    miou_table = []\n",
        "    pix_acc_table = []\n",
        "    for data_name in (pbar := tqdm(evaluate_data)):\n",
        "      img_path = root +'val_images/' + data_name + '.jpg'\n",
        "      seg_path = root + 'val_segmentations/' + data_name + '.png'\n",
        "      # Read Image and Ground truth mask\n",
        "      image = cv2.imread(img_path)\n",
        "      if image is None:\n",
        "          print(\"\\nimage is None\", data_name)\n",
        "          continue\n",
        "      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "      mask_gt = cv2.imread(seg_path)\n",
        "      if mask_gt is None:\n",
        "          print(\"\\nmask_gt is None\", data_name)\n",
        "          continue\n",
        "\n",
        "      # Generate masks for all object by SAM\n",
        "      masks = sam_generator.generate(image)\n",
        "\n",
        "      # Cut out all masks\n",
        "      input_img = Image.open(img_path)\n",
        "      cropped_boxes = []\n",
        "\n",
        "      for mask in masks:\n",
        "        crop_box = convert_box_xywh_to_xyxy(mask['bbox'])\n",
        "        if(crop_box[0]==crop_box[2] or crop_box[1]==crop_box[3]):\n",
        "          continue\n",
        "        cropped_boxes.append(segment_image(input_img, mask[\"segmentation\"]).crop(crop_box))\n",
        "\n",
        "      preprocessed_images = [preprocess(img).to(device) for img in cropped_boxes]\n",
        "      stacked_images = torch.stack(preprocessed_images)\n",
        "      image_features = model.encode_image(stacked_images)\n",
        "\n",
        "      # Get Mask By Label Id\n",
        "      anns = gt_to_anns_of_label_mask(mask_gt)\n",
        "      img_miou_sum , img_pixacc_sum, num_class = 0, 0, len(anns)\n",
        "      predict_masks = {}\n",
        "      for ann in anns:\n",
        "        scores = retriev(image_features, prompt+LABELS[ann['label']])\n",
        "        ## Get Label Index with Highest Score\n",
        "        predict_idx = np.argmax(scores.cpu())\n",
        "        predict_idx = predict_idx.cpu()\n",
        "        predict_masks[ann['label']] = masks[predict_idx][\"segmentation\"]\n",
        "      miou, pix_acc = compute_metric(data_name, predict_masks, mask_gt[:,:,0])\n",
        "      miou_table.append(miou)\n",
        "      pix_acc_table.append(pix_acc)\n",
        "    return miou_table, pix_acc_table\n",
        "\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    print(miou_table)\n",
        "    print(pix_acc_table)\n",
        "\n",
        "\n",
        "def export_csv(miou_table, pix_acc_table, miou_csv_name=\"random_miou.csv\", pix_acc_csv_name=\"random_pix_acc.csv\", export = True):\n",
        "  miou_table_ = pd.DataFrame(miou_table, columns=miou_table[0].keys()).set_index('name')\n",
        "  pix_acc_table_ = pd.DataFrame(pix_acc_table, columns=pix_acc_table[0].keys()).set_index('name')\n",
        "  if export:\n",
        "    miou_table_.to_csv('clip_sam_result/'+miou_csv_name)\n",
        "    pix_acc_table_.to_csv('clip_sam_result/'+pix_acc_csv_name)\n",
        "\n",
        "\n",
        "  # print('miou:\\n', miou_table_.mean(axis=None))\n",
        "  # print('miou per class:\\n', miou_table_.mean())\n",
        "  print()\n",
        "  # print('pix_acc:\\n', pix_acc_table_.mean(axis=None))\n",
        "  # print('pix_acc per class:\\n', pix_acc_table_.mean())\n",
        "  return miou_table_.mean(), pix_acc_table_.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Jn8u7us3nPC6"
      },
      "outputs": [],
      "source": [
        "mask_generator_default = SamAutomaticMaskGenerator(\n",
        "    sam,\n",
        "    points_per_side = 32,\n",
        "    points_per_batch = 64,\n",
        "    pred_iou_thresh = 0.88,\n",
        "    stability_score_thresh = 0.95,\n",
        "    stability_score_offset = 1.0,\n",
        "    box_nms_thresh = 0.7,\n",
        "    crop_n_layers = 0,\n",
        "    crop_nms_thresh = 0.7,\n",
        "    crop_overlap_ratio = 512 / 1500,\n",
        "    crop_n_points_downscale_factor = 1,\n",
        "    point_grids = None,\n",
        "    min_mask_region_area = 0,\n",
        "    output_mode = \"binary_mask\"\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4VJ3lX1tTS4"
      },
      "source": [
        "### Evaluate on Default Sam Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xm-EXhUspdoe",
        "outputId": "e39cc3b3-d6b8-4ca2-cda3-1909e47076be"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 4/10 [00:15<00:23,  3.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2 0 [20, 98, 22, 98]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 4/10 [00:16<00:24,  4.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacty of 11.74 GiB of which 151.94 MiB is free. Including non-PyTorch memory, this process has 10.74 GiB memory in use. Of the allocated memory 7.40 GiB is allocated by PyTorch, and 2.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "[{'name': '100034_483681', 'Background': 0.0, 'Hat': 0.5926424405563033, 'Hair': nan, 'Glove': nan, 'Sunglasses': nan, 'UpperClothes': 0.849594868332208, 'Dress': 0.0, 'Coat': nan, 'Socks': 0.2830278505117829, 'Pants': nan, 'Jumpsuits': nan, 'Scarf': nan, 'Skirt': nan, 'Face': 0.0, 'Left-arm': 0.2974566575728241, 'Right-arm': nan, 'Left-leg': 0.0, 'Right-leg': 0.0001314809133540781, 'Left-shoe': 0.5409395973154363, 'Right-shoe': 0.7122153209109731}, {'name': '10005_205677', 'Background': 0.0, 'Hat': 0.8782051282051282, 'Hair': nan, 'Glove': nan, 'Sunglasses': nan, 'UpperClothes': 0.0, 'Dress': nan, 'Coat': 0.0, 'Socks': nan, 'Pants': nan, 'Jumpsuits': nan, 'Scarf': nan, 'Skirt': nan, 'Face': 0.0, 'Left-arm': 0.0, 'Right-arm': nan, 'Left-leg': nan, 'Right-leg': nan, 'Left-shoe': nan, 'Right-shoe': nan}, {'name': '100142_449784', 'Background': 0.0, 'Hat': nan, 'Hair': 0.0, 'Glove': nan, 'Sunglasses': nan, 'UpperClothes': 0.0, 'Dress': nan, 'Coat': nan, 'Socks': nan, 'Pants': 0.9249565145365692, 'Jumpsuits': nan, 'Scarf': nan, 'Skirt': nan, 'Face': 0.0, 'Left-arm': 0.0, 'Right-arm': 0.0, 'Left-leg': 0.0, 'Right-leg': 0.0, 'Left-shoe': 0.0, 'Right-shoe': 0.0}, {'name': '10014_1211482', 'Background': 0.0, 'Hat': 0.0, 'Hair': nan, 'Glove': nan, 'Sunglasses': nan, 'UpperClothes': 0.019891500904159132, 'Dress': nan, 'Coat': 0.9363874345549739, 'Socks': nan, 'Pants': 0.9305716269382087, 'Jumpsuits': nan, 'Scarf': nan, 'Skirt': nan, 'Face': 0.8034744842562432, 'Left-arm': 0.8519417475728155, 'Right-arm': nan, 'Left-leg': nan, 'Right-leg': nan, 'Left-shoe': nan, 'Right-shoe': 0.0}]\n",
            "[{'name': '100034_483681', 'Background': 0.36846563254573267, 'Hat': 0.9885210047913428, 'Hair': nan, 'Glove': nan, 'Sunglasses': nan, 'UpperClothes': 0.9887359198998749, 'Dress': 0.8952478476883984, 'Coat': nan, 'Socks': 0.9619220995941897, 'Pants': nan, 'Jumpsuits': nan, 'Scarf': nan, 'Skirt': nan, 'Face': 0.9575605870975082, 'Left-arm': 0.8990783934463534, 'Right-arm': nan, 'Left-leg': 0.6946688411018824, 'Right-leg': 0.7115839243498818, 'Left-shoe': 0.9827056547957674, 'Right-shoe': 0.9947282588083589}, {'name': '10005_205677', 'Background': 0.3484276729559748, 'Hat': 0.9914645103324349, 'Hair': nan, 'Glove': nan, 'Sunglasses': nan, 'UpperClothes': 0.9299865229110512, 'Dress': nan, 'Coat': 0.7178796046720575, 'Socks': nan, 'Pants': nan, 'Jumpsuits': nan, 'Scarf': nan, 'Skirt': nan, 'Face': 0.6248876909254267, 'Left-arm': 0.9239667565139263, 'Right-arm': nan, 'Left-leg': nan, 'Right-leg': nan, 'Left-shoe': nan, 'Right-shoe': nan}, {'name': '100142_449784', 'Background': 0.5345836628629533, 'Hat': nan, 'Hair': 0.9732459609083719, 'Glove': nan, 'Sunglasses': nan, 'UpperClothes': 0.7542085640040673, 'Dress': nan, 'Coat': nan, 'Socks': nan, 'Pants': 0.9897638684894362, 'Jumpsuits': nan, 'Scarf': nan, 'Skirt': nan, 'Face': 0.9564568975257033, 'Left-arm': 0.9763190599932211, 'Right-arm': 0.9453508078183256, 'Left-leg': 0.9622302564681957, 'Right-leg': 0.9670319737882725, 'Left-shoe': 0.9670545701050729, 'Right-shoe': 0.9586148457801379}, {'name': '10014_1211482', 'Background': 0.525021055586749, 'Hat': 0.9257790567097136, 'Hair': nan, 'Glove': nan, 'Sunglasses': nan, 'UpperClothes': 0.9429393599101629, 'Dress': nan, 'Coat': 0.9829449747332959, 'Socks': nan, 'Pants': 0.9894722066254913, 'Jumpsuits': nan, 'Scarf': nan, 'Skirt': nan, 'Face': 0.9936482313307131, 'Left-arm': 0.9978593486805165, 'Right-arm': nan, 'Left-leg': nan, 'Right-leg': nan, 'Left-shoe': nan, 'Right-shoe': 0.9497122403144301}]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "cannot unpack non-iterable NoneType object",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/hongxin/Documents/segment-anything/notebooks/eval/CLIP_+_SAM_local_machine.ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hongxin/Documents/segment-anything/notebooks/eval/CLIP_%2B_SAM_local_machine.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m miou_table, pix_acc_table \u001b[39m=\u001b[39m evaluate(mask_generator_default,\u001b[39m10\u001b[39m)\n",
            "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
          ]
        }
      ],
      "source": [
        "miou_table, pix_acc_table = evaluate(mask_generator_default)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QG-WXukQ2Mgv",
        "outputId": "4cf279e7-cdfa-42ee-b953-916ef32ecbb6"
      },
      "outputs": [],
      "source": [
        "miou_mean_default_100, pixacc_mean_default_100 = export_csv(miou_table, pix_acc_table, miou_csv_name=\"random_miou_default_100.csv\", pix_acc_csv_name=\"random_pix_acc_default_100.csv\")\n",
        "print('miou per class\\n', miou_mean_default_100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGX4yEU6w1LX"
      },
      "source": [
        "### Evaluate on Other Sams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whqlE3_q7TcT"
      },
      "source": [
        "#### 1: points_per_side"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "25-jSG0s3ePJ"
      },
      "outputs": [],
      "source": [
        "mask_generator_1 = SamAutomaticMaskGenerator(\n",
        "    sam,\n",
        "    points_per_side = 16,\n",
        "    points_per_batch = 64,\n",
        "    pred_iou_thresh = 0.88,\n",
        "    stability_score_thresh = 0.95,\n",
        "    stability_score_offset = 1.0,\n",
        "    box_nms_thresh = 0.7,\n",
        "    crop_n_layers = 0,\n",
        "    crop_nms_thresh = 0.7,\n",
        "    crop_overlap_ratio = 512 / 1500,\n",
        "    crop_n_points_downscale_factor = 1,\n",
        "    point_grids = None,\n",
        "    min_mask_region_area = 0,\n",
        "    output_mode = \"binary_mask\"\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDYvTDxXplEm",
        "outputId": "b8362abf-fa2a-4aa6-9b8a-4b96a2051fd3"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/CSCI567/segment-anything/datasets/people_poses/val_id.txt'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/hongxin/Documents/segment-anything/notebooks/eval/CLIP_+_SAM_local_machine.ipynb Cell 31\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hongxin/Documents/segment-anything/notebooks/eval/CLIP_%2B_SAM_local_machine.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m miou_table_1, pix_acc_table_1 \u001b[39m=\u001b[39m evaluate(mask_generator_1,\u001b[39m10\u001b[39;49m)\n",
            "\u001b[1;32m/home/hongxin/Documents/segment-anything/notebooks/eval/CLIP_+_SAM_local_machine.ipynb Cell 31\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hongxin/Documents/segment-anything/notebooks/eval/CLIP_%2B_SAM_local_machine.ipynb#X44sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m root \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/content/drive/MyDrive/CSCI567/segment-anything/datasets/people_poses/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hongxin/Documents/segment-anything/notebooks/eval/CLIP_%2B_SAM_local_machine.ipynb#X44sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mThe object of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hongxin/Documents/segment-anything/notebooks/eval/CLIP_%2B_SAM_local_machine.ipynb#X44sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(root, \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mval_id.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m), \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m lf:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hongxin/Documents/segment-anything/notebooks/eval/CLIP_%2B_SAM_local_machine.ipynb#X44sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     data_list \u001b[39m=\u001b[39m [ s\u001b[39m.\u001b[39mstrip() \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m lf\u001b[39m.\u001b[39mreadlines() ]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hongxin/Documents/segment-anything/notebooks/eval/CLIP_%2B_SAM_local_machine.ipynb#X44sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[0;32m~/anaconda3/envs/CSCI567/lib/python3.11/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/CSCI567/segment-anything/datasets/people_poses/val_id.txt'"
          ]
        }
      ],
      "source": [
        "miou_table_1, pix_acc_table_1 = evaluate(mask_generator_1,10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEJ2gvp33Yyg",
        "outputId": "ea4c336b-2a4d-4a12-9676-899158305a3a"
      },
      "outputs": [],
      "source": [
        "miou_mean_1_100, pixacc_mean_1_100 = export_csv(miou_table_1, pix_acc_table_1, miou_csv_name=\"random_miou_1_100.csv\", pix_acc_csv_name=\"random_pix_acc_1_100.csv\", export = False)\n",
        "# print('miou per class\\n', miou_mean_1_100)\n",
        "print(\"miou per class difference\\n\", miou_mean_1_100-miou_mean_default_100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAl99w-W7WHK"
      },
      "source": [
        "#### 2: points_per_side"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8O6-EDti7Xyr"
      },
      "outputs": [],
      "source": [
        "mask_generator_2 = SamAutomaticMaskGenerator(\n",
        "    sam,\n",
        "    points_per_side = 64,\n",
        "    points_per_batch = 64,\n",
        "    pred_iou_thresh = 0.88,\n",
        "    stability_score_thresh = 0.95,\n",
        "    stability_score_offset = 1.0,\n",
        "    box_nms_thresh = 0.7,\n",
        "    crop_n_layers = 0,\n",
        "    crop_nms_thresh = 0.7,\n",
        "    crop_overlap_ratio = 512 / 1500,\n",
        "    crop_n_points_downscale_factor = 1,\n",
        "    point_grids = None,\n",
        "    min_mask_region_area = 0,\n",
        "    output_mode = \"binary_mask\"\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zLGOrGF7ZV8",
        "outputId": "1b609640-4599-45e4-a356-d39c6735bf71"
      },
      "outputs": [],
      "source": [
        "miou_table_2, pix_acc_table_2 = evaluate(mask_generator_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4G6uR9BwHWkV",
        "outputId": "23cb95dc-c12f-4629-db7c-67d324c328f5"
      },
      "outputs": [],
      "source": [
        "miou_mean_2_100, pixacc_mean_2_100 = export_csv(miou_table_2, pix_acc_table_2, miou_csv_name=\"random_miou_2_100.csv\", pix_acc_csv_name=\"random_pix_acc_2_100.csv\", export = False)\n",
        "# print('miou per class\\n', miou_mean_1_100)\n",
        "print(\"miou per class difference\\n\", miou_mean_2_100-miou_mean_default_100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2ipqkAox7ox"
      },
      "source": [
        "#### 3: some thresholds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_101cl2x2Sp"
      },
      "outputs": [],
      "source": [
        "mask_generator_3 = SamAutomaticMaskGenerator(\n",
        "    sam,\n",
        "    points_per_side = 32,\n",
        "    points_per_batch = 64,\n",
        "    pred_iou_thresh = 0.95,\n",
        "    stability_score_thresh = 0.95,\n",
        "    stability_score_offset = 1.0,\n",
        "    box_nms_thresh = 0.9,\n",
        "    crop_n_layers = 0,\n",
        "    crop_nms_thresh = 0.9,\n",
        "    crop_overlap_ratio = 512 / 1500,\n",
        "    crop_n_points_downscale_factor = 1,\n",
        "    point_grids = None,\n",
        "    min_mask_region_area = 0,\n",
        "    output_mode = \"binary_mask\"\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ltuGlO6yOiD",
        "outputId": "1ebc8d94-4b03-46b3-de7c-c629a9f332f2"
      },
      "outputs": [],
      "source": [
        "miou_table_3, pix_acc_table_3 = evaluate(mask_generator_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mtjotX63WHx",
        "outputId": "edb8b935-5641-4e09-f72e-db68ff985af3"
      },
      "outputs": [],
      "source": [
        "miou_mean_3_100, pixacc_mean_3_100 = export_csv(miou_table_3, pix_acc_table_3, miou_csv_name=\"random_miou_3_100.csv\", pix_acc_csv_name=\"random_pix_acc_3_100.csv\", export = False)\n",
        "# print('miou per class\\n', miou_mean_1_100)\n",
        "print(\"miou per class difference\\n\", miou_mean_3_100-miou_mean_default_100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbWix8fyfW2c"
      },
      "source": [
        "#### 4: Hyperparameters in the predictor notebook: https://colab.research.google.com/drive/12yvkr9VomnceYOdQ5dbdoaPIqXRiOPmb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Cyhrf2pfYpP"
      },
      "outputs": [],
      "source": [
        "mask_generator_4 = SamAutomaticMaskGenerator(\n",
        "    sam,\n",
        "    points_per_side = 32,\n",
        "    points_per_batch = 64,\n",
        "    pred_iou_thresh = 0.86,\n",
        "    stability_score_thresh = 0.92,\n",
        "    stability_score_offset = 1.0,\n",
        "    box_nms_thresh = 0.7,\n",
        "    crop_n_layers = 1,\n",
        "    crop_nms_thresh = 0.7,\n",
        "    crop_overlap_ratio = 512 / 1500,\n",
        "    crop_n_points_downscale_factor = 2,\n",
        "    point_grids = None,\n",
        "    min_mask_region_area = 100,\n",
        "    output_mode = \"binary_mask\"\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0Apm__qf2jF",
        "outputId": "7cd5f3fa-5ee8-4e05-e7a4-c0e4972243d9"
      },
      "outputs": [],
      "source": [
        "miou_table_4, pix_acc_table_4 = evaluate(mask_generator_4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOHkL9SG6XU7",
        "outputId": "2b8d8375-f32c-4467-ebd6-ec423f4b6ad8"
      },
      "outputs": [],
      "source": [
        "miou_mean_4_100, pixacc_mean_4_100 = export_csv(miou_table_4, pix_acc_table_4, miou_csv_name=\"random_miou_4_100.csv\", pix_acc_csv_name=\"random_pix_acc_4_100.csv\")\n",
        "print(\"miou per class difference\\n\", miou_mean_4_100-miou_mean_default_100)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
