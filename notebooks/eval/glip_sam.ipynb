{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfN3e_6aUnDe"
      },
      "source": [
        "# Using CLIPSeg with Hugging Face Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JE8CVZ86W2Ru"
      },
      "source": [
        "Using Hugging Face Transformers, you can easily download and run a pre-trained CLIPSeg model on your images. Let’s start by installing transformers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jTjY5YtQi1kT",
        "outputId": "aa736c57-243d-43aa-8f09-a084c419a8b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n",
            "gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Copyright (C) 2021 Free Software Foundation, Inc.\n",
            "This is free software; see the source for copying conditions.  There is NO\n",
            "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
            "\n",
            "Collecting openmim\n",
            "  Downloading openmim-0.3.9-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click in /usr/local/lib/python3.10/dist-packages (from openmim) (8.1.7)\n",
            "Collecting colorama (from openmim)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting model-index (from openmim)\n",
            "  Downloading model_index-0.1.11-py3-none-any.whl (34 kB)\n",
            "Collecting opendatalab (from openmim)\n",
            "  Downloading opendatalab-0.0.10-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from openmim) (1.5.3)\n",
            "Requirement already satisfied: pip>=19.3 in /usr/local/lib/python3.10/dist-packages (from openmim) (23.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from openmim) (2.31.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from openmim) (13.7.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from openmim) (0.9.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from model-index->openmim) (6.0.1)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from model-index->openmim) (3.5.1)\n",
            "Collecting ordered-set (from model-index->openmim)\n",
            "  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
            "Collecting pycryptodome (from opendatalab->openmim)\n",
            "  Downloading pycryptodome-3.19.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatalab->openmim) (4.66.1)\n",
            "Collecting openxlab (from opendatalab->openmim)\n",
            "  Downloading openxlab-0.0.29-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.4/297.4 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->openmim) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->openmim) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->openmim) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->openmim) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->openmim) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->openmim) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas->openmim) (1.23.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->openmim) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->openmim) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->openmim) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->openmim) (1.16.0)\n",
            "Collecting oss2~=2.17.0 (from openxlab->opendatalab->openmim)\n",
            "  Downloading oss2-2.17.0.tar.gz (259 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.5/259.5 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting requests (from openmim)\n",
            "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rich (from openmim)\n",
            "  Downloading rich-13.4.2-py3-none-any.whl (239 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.4/239.4 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setuptools~=60.2.0 (from openxlab->opendatalab->openmim)\n",
            "  Downloading setuptools-60.2.0-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.1/953.1 kB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm (from opendatalab->openmim)\n",
            "  Downloading tqdm-4.65.2-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.21.1 (from requests->openmim)\n",
            "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting crcmod>=1.7 (from oss2~=2.17.0->openxlab->opendatalab->openmim)\n",
            "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting aliyun-python-sdk-kms>=2.4.1 (from oss2~=2.17.0->openxlab->opendatalab->openmim)\n",
            "  Downloading aliyun_python_sdk_kms-2.16.2-py2.py3-none-any.whl (94 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.0/94.0 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aliyun-python-sdk-core>=2.13.12 (from oss2~=2.17.0->openxlab->opendatalab->openmim)\n",
            "  Downloading aliyun-python-sdk-core-2.14.0.tar.gz (443 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.0/443.0 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jmespath<1.0.0,>=0.9.3 (from aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim)\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: cryptography>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim) (41.0.7)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=2.6.0->aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=2.6.0->aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim) (2.21)\n",
            "Building wheels for collected packages: oss2, aliyun-python-sdk-core, crcmod\n",
            "  Building wheel for oss2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for oss2: filename=oss2-2.17.0-py3-none-any.whl size=112371 sha256=2655bef16980aff8416b3a8af3eb9deae15dc2a49a60c6f9f9e93d58a20699d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/04/7b/7e61b8157fdf211c5131375240d0d86ca82e2a88ead9672c88\n",
            "  Building wheel for aliyun-python-sdk-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for aliyun-python-sdk-core: filename=aliyun_python_sdk_core-2.14.0-py3-none-any.whl size=535289 sha256=a057f96f97d1e9b4b65d77707f1bef896bfbc68960f799c905054504fcdda8bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/16/3c/68/b7eab618d9f1d5e7d386296f1e07e2cf36aaa1eb5161885038\n",
            "  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for crcmod: filename=crcmod-1.7-cp310-cp310-linux_x86_64.whl size=31408 sha256=4d2c29764c9e8b314fc5cb85d4911a3d4f25b590c9318b694aa7efb6230d45b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/4c/07/72215c529bd59d67e3dac29711d7aba1b692f543c808ba9e86\n",
            "Successfully built oss2 aliyun-python-sdk-core crcmod\n",
            "Installing collected packages: crcmod, urllib3, tqdm, setuptools, pycryptodome, ordered-set, jmespath, colorama, rich, requests, model-index, aliyun-python-sdk-core, aliyun-python-sdk-kms, oss2, openxlab, opendatalab, openmim\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.1\n",
            "    Uninstalling tqdm-4.66.1:\n",
            "      Successfully uninstalled tqdm-4.66.1\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 67.7.2\n",
            "    Uninstalling setuptools-67.7.2:\n",
            "      Successfully uninstalled setuptools-67.7.2\n",
            "  Attempting uninstall: rich\n",
            "    Found existing installation: rich 13.7.0\n",
            "    Uninstalling rich-13.7.0:\n",
            "      Successfully uninstalled rich-13.7.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "cvxpy 1.3.2 requires setuptools>65.5.1, but you have setuptools 60.2.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.28.2 which is incompatible.\n",
            "yfinance 0.2.32 requires requests>=2.31, but you have requests 2.28.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aliyun-python-sdk-core-2.14.0 aliyun-python-sdk-kms-2.16.2 colorama-0.4.6 crcmod-1.7 jmespath-0.10.0 model-index-0.1.11 opendatalab-0.0.10 openmim-0.3.9 openxlab-0.0.29 ordered-set-4.1.0 oss2-2.17.0 pycryptodome-3.19.0 requests-2.28.2 rich-13.4.2 setuptools-60.2.0 tqdm-4.65.2 urllib3-1.26.18\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "pkg_resources",
                  "setuptools"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.openmmlab.com/mmcv/dist/cu118/torch2.1.0/index.html\n",
            "Collecting mmengine>=0.7.0\n",
            "  Downloading mmengine-0.10.1-py3-none-any.whl (450 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m450.3/450.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting addict (from mmengine>=0.7.0)\n",
            "  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mmengine>=0.7.0) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mmengine>=0.7.0) (1.23.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from mmengine>=0.7.0) (6.0.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from mmengine>=0.7.0) (13.4.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from mmengine>=0.7.0) (2.3.0)\n",
            "Collecting yapf (from mmengine>=0.7.0)\n",
            "  Downloading yapf-0.40.2-py3-none-any.whl (254 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.7/254.7 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-python>=3 in /usr/local/lib/python3.10/dist-packages (from mmengine>=0.7.0) (4.8.0.76)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine>=0.7.0) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine>=0.7.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine>=0.7.0) (4.45.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine>=0.7.0) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine>=0.7.0) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine>=0.7.0) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine>=0.7.0) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine>=0.7.0) (2.8.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->mmengine>=0.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->mmengine>=0.7.0) (2.16.1)\n",
            "Requirement already satisfied: importlib-metadata>=6.6.0 in /usr/local/lib/python3.10/dist-packages (from yapf->mmengine>=0.7.0) (6.8.0)\n",
            "Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from yapf->mmengine>=0.7.0) (4.0.0)\n",
            "Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from yapf->mmengine>=0.7.0) (2.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.6.0->yapf->mmengine>=0.7.0) (3.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->mmengine>=0.7.0) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mmengine>=0.7.0) (1.16.0)\n",
            "Installing collected packages: addict, yapf, mmengine\n",
            "Successfully installed addict-2.4.0 mmengine-0.10.1 yapf-0.40.2\n",
            "Looking in links: https://download.openmmlab.com/mmcv/dist/cu118/torch2.1.0/index.html\n",
            "Collecting mmcv>=2.0.0rc4\n",
            "  Downloading https://download.openmmlab.com/mmcv/dist/cu118/torch2.1.0/mmcv-2.1.0-cp310-cp310-manylinux1_x86_64.whl (99.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.3/99.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: addict in /usr/local/lib/python3.10/dist-packages (from mmcv>=2.0.0rc4) (2.4.0)\n",
            "Requirement already satisfied: mmengine>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from mmcv>=2.0.0rc4) (0.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mmcv>=2.0.0rc4) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mmcv>=2.0.0rc4) (23.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from mmcv>=2.0.0rc4) (9.4.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from mmcv>=2.0.0rc4) (6.0.1)\n",
            "Requirement already satisfied: yapf in /usr/local/lib/python3.10/dist-packages (from mmcv>=2.0.0rc4) (0.40.2)\n",
            "Requirement already satisfied: opencv-python>=3 in /usr/local/lib/python3.10/dist-packages (from mmcv>=2.0.0rc4) (4.8.0.76)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mmengine>=0.3.0->mmcv>=2.0.0rc4) (3.7.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from mmengine>=0.3.0->mmcv>=2.0.0rc4) (13.4.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from mmengine>=0.3.0->mmcv>=2.0.0rc4) (2.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=6.6.0 in /usr/local/lib/python3.10/dist-packages (from yapf->mmcv>=2.0.0rc4) (6.8.0)\n",
            "Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from yapf->mmcv>=2.0.0rc4) (4.0.0)\n",
            "Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from yapf->mmcv>=2.0.0rc4) (2.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.6.0->yapf->mmcv>=2.0.0rc4) (3.17.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine>=0.3.0->mmcv>=2.0.0rc4) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine>=0.3.0->mmcv>=2.0.0rc4) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine>=0.3.0->mmcv>=2.0.0rc4) (4.45.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine>=0.3.0->mmcv>=2.0.0rc4) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine>=0.3.0->mmcv>=2.0.0rc4) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine>=0.3.0->mmcv>=2.0.0rc4) (2.8.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->mmengine>=0.3.0->mmcv>=2.0.0rc4) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->mmengine>=0.3.0->mmcv>=2.0.0rc4) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->mmengine>=0.3.0->mmcv>=2.0.0rc4) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mmengine>=0.3.0->mmcv>=2.0.0rc4) (1.16.0)\n",
            "Installing collected packages: mmcv\n",
            "Successfully installed mmcv-2.1.0\n",
            "Cloning into 'mmdetection'...\n",
            "remote: Enumerating objects: 37679, done.\u001b[K\n",
            "remote: Counting objects: 100% (280/280), done.\u001b[K\n",
            "remote: Compressing objects: 100% (204/204), done.\u001b[K\n",
            "remote: Total 37679 (delta 114), reused 151 (delta 72), pack-reused 37399\u001b[K\n",
            "Receiving objects: 100% (37679/37679), 63.04 MiB | 21.80 MiB/s, done.\n",
            "Resolving deltas: 100% (25955/25955), done.\n",
            "/content/mmdetection\n",
            "Obtaining file:///content/mmdetection\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mmdet==3.2.0) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mmdet==3.2.0) (1.23.5)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (from mmdet==3.2.0) (2.0.7)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from mmdet==3.2.0) (1.11.4)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.10/dist-packages (from mmdet==3.2.0) (2.0.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from mmdet==3.2.0) (1.16.0)\n",
            "Collecting terminaltables (from mmdet==3.2.0)\n",
            "  Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from mmdet==3.2.0) (4.65.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmdet==3.2.0) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmdet==3.2.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmdet==3.2.0) (4.45.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmdet==3.2.0) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmdet==3.2.0) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmdet==3.2.0) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmdet==3.2.0) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmdet==3.2.0) (2.8.2)\n",
            "Installing collected packages: terminaltables, mmdet\n",
            "  Running setup.py develop for mmdet\n",
            "Successfully installed mmdet-3.2.0 terminaltables-3.1.10\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.23.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for segment-anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.8/367.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.5/68.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.2/72.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.3/158.3 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "yfinance 0.2.32 requires requests>=2.31, but you have requests 2.28.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m--2023-12-04 15:25:54--  https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.35.7.50, 13.35.7.128, 13.35.7.82, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.35.7.50|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2564550879 (2.4G) [binary/octet-stream]\n",
            "Saving to: ‘sam_vit_h_4b8939.pth’\n",
            "\n",
            "sam_vit_h_4b8939.pt 100%[===================>]   2.39G   132MB/s    in 18s     \n",
            "\n",
            "2023-12-04 15:26:12 (135 MB/s) - ‘sam_vit_h_4b8939.pth’ saved [2564550879/2564550879]\n",
            "\n",
            "Found existing installation: supervision 0.16.0\n",
            "Uninstalling supervision-0.16.0:\n",
            "  Successfully uninstalled supervision-0.16.0\n"
          ]
        }
      ],
      "source": [
        "# Check nvcc version\n",
        "!nvcc -V\n",
        "# Check GCC version\n",
        "!gcc --version\n",
        "\n",
        "# install dependencies: (use cu111 because colab has CUDA 11.1)\n",
        "%pip install -U openmim\n",
        "!mim install \"mmengine>=0.7.0\"\n",
        "!mim install \"mmcv>=2.0.0rc4\"\n",
        "\n",
        "# Install mmdetection\n",
        "!rm -rf mmdetection\n",
        "!git clone https://github.com/open-mmlab/mmdetection.git\n",
        "%cd mmdetection\n",
        "\n",
        "%pip install -e .\n",
        "\n",
        "!pip install torch opencv-python Pillow\n",
        "!pip install -q 'git+https://github.com/facebookresearch/segment-anything.git'\n",
        "!pip install -q jupyter_bbox_widget roboflow dataclasses-json supervision\n",
        "!pip install -q transformers\n",
        "!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
        "\n",
        "!pip uninstall -y supervision\n",
        "!pip install -q supervision==0.6.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2MF3RpSxaw-",
        "outputId": "9b92267d-d835-4ef7-e38d-c62f3f0b2e12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "0.6.0\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append('/content')\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import supervision as sv\n",
        "print(sv.__version__)\n",
        "import torch\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2jHdaHJ0io7",
        "outputId": "01ec0729-5e06-4395-98b2-f5e02b4d5f8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/checkpoints’: File exists\n",
            "processing glip_atss_swin-t_fpn_dyhead_pretrain_obj365-goldg-cc3m-sub...\n",
            "\u001b[32mglip_tiny_mmdet-c24ce662.pth exists in /content/checkpoints\u001b[0m\n",
            "\u001b[32mSuccessfully dumped glip_atss_swin-t_fpn_dyhead_pretrain_obj365-goldg-cc3m-sub.py to /content/checkpoints\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#GLIP TINY\n",
        "model_name = 'glip_atss_swin-t_fpn_dyhead_pretrain_obj365-goldg-cc3m-sub'\n",
        "checkpoint =  '/content/checkpoints/glip_tiny_mmdet-c24ce662.pth'\n",
        "!mkdir /content/checkpoints\n",
        "!mim download mmdet --config {model_name} --dest /content/checkpoints\n",
        "\n",
        "\n",
        "#GLIP LARGE\n",
        "# model_name = 'glip_atss_swin-l_fpn_dyhead_pretrain_mixeddata'\n",
        "# checkpoint =  '/content/checkpoints/glip_l_mmdet-abfe026b.pth'\n",
        "# !mkdir /content/checkpoints\n",
        "# !mim download mmdet --config {modelName} --dest /content/checkpoints\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "BKgPlbq5jUb3",
        "outputId": "aac8234b-31ad-4fbd-bfe1-0c2c9b4f5b4b"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0d000834a549>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmmdet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapis\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDetInferencer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Set the device to be used for evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# device = 'cuda:0'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mmdet'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from mmdet.apis import DetInferencer\n",
        "\n",
        "# Set the device to be used for evaluation\n",
        "device = 'cuda:0'\n",
        "\n",
        "# Initialize the DetInferencer\n",
        "inferencer = DetInferencer(model_name, checkpoint, device)\n",
        "\n",
        "\n",
        "# #TEST\n",
        "# # Use the detector to do inference\n",
        "# img = '/content/drive/MyDrive/CSCI 567 /segment-anything/datasets/people_poses/val_images/100034_483681.jpg'\n",
        "# result = inferencer(img, texts='Socks', out_dir='./output')\n",
        "# predictions = result['predictions']\n",
        "\n",
        "# bbox = []\n",
        "# if len(predictions) > 0:\n",
        "#   bboxVal = predictions[0]['bboxes']\n",
        "#   xMin = 0.0\n",
        "#   yMin = 0.0\n",
        "#   xMax = 0.0\n",
        "#   yMax = 0.0\n",
        "#   if len(bboxVal) > 0:\n",
        "#     xMin, yMin, xMax, yMax = bboxVal[0]\n",
        "#   i = 0\n",
        "#   for box in bboxVal:\n",
        "#     if predictions[0]['scores'][i] > 0.5:\n",
        "#       print(box)\n",
        "#       x1, y1, x2, y2 = box\n",
        "#       if x1 < xMin:\n",
        "#         xMin = x1\n",
        "#       if y1 < yMin:\n",
        "#         yMin = y1\n",
        "#       if x2 > xMax:\n",
        "#         xMax = x2\n",
        "#       if y2 > yMax:\n",
        "#         yMax = y2\n",
        "#     i += 1\n",
        "\n",
        "#   bbox = [xMin, yMin, xMax, yMax]\n",
        "\n",
        "\n",
        "# print(\"final\")\n",
        "# print(bbox)\n",
        "\n",
        "# # Show the output image\n",
        "# from PIL import Image\n",
        "# Image.open('./output/vis/100034_483681.jpg')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjnCAKnaU04t"
      },
      "source": [
        "## Text prompting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qc4IuietEqr1"
      },
      "source": [
        "Let’s start by defining some text categories we want to segment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVWkP-j0W8WO"
      },
      "source": [
        "Now that we have our inputs, we can process them and input them to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfCjKfgtW1_M"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "def getPredictions(promptValue, image, imagePath):\n",
        "  prompts = [promptValue]\n",
        "  # inputs = processor(text=prompts, images=[image] * len(prompts), padding=\"max_length\", return_tensors=\"pt\")\n",
        "  # # predict\n",
        "  # with torch.no_grad():\n",
        "  #   outputs = model(**inputs)\n",
        "  BOX_TRESHOLD = 0.35\n",
        "  TEXT_TRESHOLD = 0.25\n",
        "  #image = cv2.imread(SOURCE_IMAGE_PATH)\n",
        "\n",
        "\n",
        "  result = inferencer(imagePath, texts= promptValue)\n",
        "  predictions = result['predictions']\n",
        "\n",
        "  bbox = []\n",
        "  if len(predictions) > 0:\n",
        "    bboxVal = predictions[0]['bboxes']\n",
        "    print(predictions[0]['scores'])\n",
        "    xMin = 0.0\n",
        "    yMin = 0.0\n",
        "    xMax = 0.0\n",
        "    yMax = 0.0\n",
        "    if len(bboxVal) > 0:\n",
        "      xMin, yMin, xMax, yMax = bboxVal[0]\n",
        "    i = 0\n",
        "    for box in bboxVal:\n",
        "      if predictions[0]['scores'][i] > 0.5:\n",
        "        x1, y1, x2, y2 = box\n",
        "        if x1 < xMin:\n",
        "          xMin = x1\n",
        "        if y1 < yMin:\n",
        "          yMin = y1\n",
        "        if x2 > xMax:\n",
        "          xMax = x2\n",
        "        if y2 > yMax:\n",
        "          yMax = y2\n",
        "      i += 1\n",
        "    bbox = [xMin, yMin, xMax, yMax]\n",
        "\n",
        "\n",
        "  if len(bbox) > 0:\n",
        "    return bbox\n",
        "  return [0, 0, 0, 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kphdXZGrc38y"
      },
      "source": [
        "# END OF CHANGES\n",
        "NO CODE BELOW THIS SECTION WAS CHANGED\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnkj-5DcTLlh"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "def make_square_by_padding(img, fill_color=(0, 0, 0)):\n",
        "    # img = Image.open(requests.get(url, stream=True).raw)\n",
        "    width, height = img.size\n",
        "\n",
        "    # Determine the size for the square\n",
        "    new_size = max(width, height)\n",
        "\n",
        "    # Create a new image with the desired size and fill color\n",
        "    new_img = Image.new(\"RGB\", (new_size, new_size), fill_color)\n",
        "\n",
        "    # Paste the original image onto the center of the new image\n",
        "    new_img.paste(img, ((new_size - width) // 2, (new_size - height) // 2))\n",
        "\n",
        "    return new_img\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Kp0mOZ7TLlh"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.ops import masks_to_boxes\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image, Image, ImageDraw\n",
        "import numpy as np\n",
        "\n",
        "def processPredictionImage(pred, img):\n",
        "    mask = torch.sigmoid(torch.reshape(torch.tensor(pred), (352, 352)))\n",
        "    # ax[i+1].imshow(mask)\n",
        "\n",
        "    processed_tensor = torch.sigmoid(torch.reshape(torch.tensor(pred), (352, 352)))\n",
        "    image_np = (processed_tensor.detach().numpy() * 255).astype(np.uint8)\n",
        "\n",
        "    _, binary_image = cv2.threshold(image_np, 127, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    desired_width, desired_height = img.size\n",
        "    resized_image = cv2.resize(binary_image, (max(desired_width, desired_height),  max(desired_width, desired_height)), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    # Get dimensions of the binary image\n",
        "    height, width = resized_image.shape[:2]\n",
        "\n",
        "    # Check if the desired crop size is smaller than the original image size\n",
        "    if desired_width <= width and desired_height <= height:\n",
        "        # Calculate the top-left corner of the crop\n",
        "        x = width // 2 - desired_width // 2\n",
        "        y = height // 2 - desired_height // 2\n",
        "\n",
        "        # Crop the image\n",
        "        cropped_image = resized_image[y:y+desired_height, x:x+desired_width]\n",
        "        return cropped_image\n",
        "\n",
        "    else:\n",
        "        print(\"failed to crop image\")\n",
        "        return resized_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhqQnM6NTLlh"
      },
      "outputs": [],
      "source": [
        "\n",
        "def getBoundingBox(predictionImage):\n",
        "    contours, _ = cv2.findContours(predictionImage, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    if contours:\n",
        "        list_of_pts = []\n",
        "        for ctr in contours:\n",
        "            list_of_pts += [pt[0] for pt in ctr]\n",
        "        ctr = np.array(list_of_pts).reshape((-1,1,2)).astype(np.int32)\n",
        "        # largest_contour = max(cv2.convexHull(ctr), key=cv2.contourArea)\n",
        "        x, y, w, h = cv2.boundingRect(cv2.convexHull(ctr))\n",
        "\n",
        "        print(f\"Bounding Box: x={x}, y={y}, width={(w)}, height={(h)}\")\n",
        "        return [x, y, (w + x), (h + y)]\n",
        "\n",
        "    else:\n",
        "        print(\"No contours found\")\n",
        "        return [0, 0, 0, 0]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S5bxvB6TLlh"
      },
      "source": [
        "# SAM Integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWvmCRbSTLlh",
        "outputId": "cf4b1ffa-dba8-4998-9b12-d4f7fa8111e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.23.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-ur6znu92\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-ur6znu92\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy (from clip==1.0)\n",
            "  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.65.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.16.0+cu118)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.28.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369537 sha256=b5703f14654a9ae1f9ffa8929c013ae8d2e43d5650d056a658320af783cbb376\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jf7atsj8/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.1.3\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "--2023-12-04 15:46:32--  https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.35.7.38, 13.35.7.50, 13.35.7.128, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.35.7.38|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2564550879 (2.4G) [binary/octet-stream]\n",
            "Saving to: ‘sam_vit_h_4b8939.pth.1’\n",
            "\n",
            "sam_vit_h_4b8939.pt 100%[===================>]   2.39G   121MB/s    in 19s     \n",
            "\n",
            "2023-12-04 15:46:51 (126 MB/s) - ‘sam_vit_h_4b8939.pth.1’ saved [2564550879/2564550879]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install torch opencv-python Pillow\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install -q 'git+https://github.com/facebookresearch/segment-anything.git'\n",
        "!pip install -q jupyter_bbox_widget roboflow dataclasses-json supervision\n",
        "\n",
        "!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gr-mAlIFTLlh"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "\n",
        "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
        "model_type = \"vit_h\"\n",
        "\n",
        "# device = \"cuda\"\n",
        "\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "# sam.to(device=device)\n",
        "\n",
        "predictor = SamPredictor(sam)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8O64h_ITLlh"
      },
      "outputs": [],
      "source": [
        "def getSAMPreditction(image, box):\n",
        "\n",
        "    image_np = np.array(image)\n",
        "    image2 = cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB)\n",
        "    predictor.set_image(image2)\n",
        "\n",
        "    input_box = np.array(box)\n",
        "    if(input_box[0] == 0 and input_box[1] == 0 and input_box[2] == 0 and input_box[3] == 0):\n",
        "        masks, _, _ = predictor.predict(\n",
        "            point_coords=None,\n",
        "            point_labels=None,\n",
        "            box=None,\n",
        "            multimask_output=False,\n",
        "        )\n",
        "    else:\n",
        "        masks, _, _ = predictor.predict(\n",
        "            point_coords=None,\n",
        "            point_labels=None,\n",
        "            box=input_box[None, :],\n",
        "            multimask_output=False,\n",
        "        )\n",
        "    return masks[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBOshUh-TLlh"
      },
      "outputs": [],
      "source": [
        "\n",
        "# numpy version\n",
        "def pixAcc(predicted, target):\n",
        "    same = (predicted == target).sum()\n",
        "    w, h = target.shape\n",
        "    print(\"Target seg shape: {}, Predicted seg shape: {}, #Same pixels: {}\".format(target.shape, predicted.shape, same))\n",
        "    return same / (w * h)\n",
        "\n",
        "# input: bool matrix\n",
        "def IOU(predicted , target):\n",
        "    intersection = np.logical_and(target, predicted).sum()\n",
        "    union = np.logical_or(target, predicted).sum()\n",
        "    if union == 0:\n",
        "        iou_score = 0\n",
        "    else :\n",
        "        iou_score = intersection / union\n",
        "    return iou_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhGmJd-KTLlh"
      },
      "outputs": [],
      "source": [
        "\n",
        "def gt_to_anns_of_label_mask(mask_gt):\n",
        "  labels = np.unique(mask_gt)\n",
        "  anns = []\n",
        "  for label in labels:\n",
        "    # skip background\n",
        "      if label == 0:\n",
        "          continue\n",
        "      mask = np.all(mask_gt == label, axis=-1)\n",
        "      anns.append({\n",
        "        'area': np.sum(mask),\n",
        "        'segmentation': mask,\n",
        "        'label': label,\n",
        "      })\n",
        "  return anns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QhkeQOnTLlh"
      },
      "source": [
        "# Final Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvSEdqao6WcA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def getFinalPredictions(images, imagePaths, promptVals):\n",
        "  preds = []\n",
        "  boundingBoxes = []\n",
        "  masks = []\n",
        "  emptyPrompt = \"\"\n",
        "  i = 0\n",
        "  for img in images:\n",
        "    imgResults = {}\n",
        "    imageBoundingBoxes = {}\n",
        "    imageMasks = {}\n",
        "    img_with_border = make_square_by_padding(img)\n",
        "\n",
        "    for prompt in promptVals:\n",
        "      # prediction = getPredictions(prompt, img)\n",
        "  #     imgResults[prompt] = prediction\n",
        "  #     predictionImage = processPredictionImage(prediction, img)\n",
        "      print(prompt)\n",
        "      bbox = getPredictions(prompt, img, imagePaths[i])\n",
        "\n",
        "      imageBoundingBoxes[prompt] = bbox\n",
        "      print(bbox)\n",
        "      if(bbox[0] == 0 and bbox[1] == 0 and bbox[2] == 0 and bbox[3] == 0 and emptyPrompt == \"\"):\n",
        "        print(\"Here\")\n",
        "        mask = getSAMPreditction(img, bbox)\n",
        "        imageMasks[prompt] = mask\n",
        "        emptyPrompt = prompt\n",
        "      elif(bbox[0] == 0 and bbox[1] == 0 and bbox[2] == 0 and bbox[3] == 0):\n",
        "        print(\"Here\")\n",
        "        imageMasks[prompt] = imageMasks[emptyPrompt]\n",
        "      else:\n",
        "        mask = getSAMPreditction(img, bbox)\n",
        "        imageMasks[prompt] = mask\n",
        "\n",
        "    emptyPrompt = \"\"\n",
        "    preds.append(imgResults)\n",
        "    boundingBoxes.append(imageBoundingBoxes)\n",
        "    masks.append(imageMasks)\n",
        "    i += 1\n",
        "  preds = np.array(preds)\n",
        "  boundingBoxes = np.array(boundingBoxes)\n",
        "  return boundingBoxes, preds, masks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGwQ0TTvTLli"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "\n",
        "def getImages(num):\n",
        "    images = []\n",
        "    imagePaths = []\n",
        "    truthMasks = []\n",
        "    data_list = []\n",
        "    root = \"\"\n",
        "    root = \"/content/drive/MyDrive/CSCI 567 /segment-anything/datasets/people_poses/\"\n",
        "    textFile = \"val_id.short.txt\"\n",
        "    imageFile = \"val_images/\"\n",
        "    segmentationFile = \"val_segmentations/\"\n",
        "    with open(os.path.join(root, f\"val_id.short.txt\"), 'r') as lf:\n",
        "        data_list = [ s.strip() for s in lf.readlines() ]\n",
        "\n",
        "    num_valid_case, sum_miou, sum_pixAcc = 0,0,0\n",
        "    try:\n",
        "        for data_name in (pbar := tqdm(data_list[:num])):\n",
        "            img_path = root + imageFile + data_name + '.jpg'\n",
        "            seg_path = root +  segmentationFile+ data_name + '.png'\n",
        "\n",
        "            # Read Image and Ground truth mask\n",
        "            img = copy.deepcopy(Image.open(img_path))\n",
        "            imagePaths.append(img_path)\n",
        "            # display(img)\n",
        "            if img is None:\n",
        "                print(\"\\nimage is None\", data_name)\n",
        "                continue\n",
        "            else:\n",
        "                images.append(img)\n",
        "            mask_gt = cv2.imread(seg_path)\n",
        "            if mask_gt is None:\n",
        "                print(\"\\nmask_gt is None\", data_name)\n",
        "                continue\n",
        "            else:\n",
        "                truthMasks.append(mask_gt)\n",
        "    except Exception as e:\n",
        "        print(\"ERROR\")\n",
        "        print(e)\n",
        "    return images, truthMasks, imagePaths\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMf9gQGQTLli"
      },
      "outputs": [],
      "source": [
        "def get_masked_image(original_image, segmentation):\n",
        "  # Visualize\n",
        "\n",
        "  overlay_image = Image.new('RGBA', original_image.size, (0, 0, 0, 0))\n",
        "  overlay_color = (255, 0, 0, 200)\n",
        "\n",
        "  draw = ImageDraw.Draw(overlay_image)\n",
        "  segmentation_mask_image = Image.fromarray(segmentation.astype('uint8') * 255)\n",
        "  draw.bitmap((0, 0), segmentation_mask_image, fill=overlay_color)\n",
        "\n",
        "  return Image.alpha_composite(original_image.convert('RGBA'), overlay_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxSyAahUTLli"
      },
      "outputs": [],
      "source": [
        "\n",
        "def evaluate(images, masks, truthMasks):\n",
        "  # Get confidence scores for each masks generated by SAM for\n",
        "  # each object label existing in the given image\n",
        "\n",
        "  results = []\n",
        "  label_name = [\"Background\",\"Hat\",\"Hair\",\"Glove\",\n",
        "        \"Sunglasses\",\"UpperClothes\",\"Dress\",\"Coat\",\"Socks\",\"Pants\",\n",
        "        \"Jumpsuits\",\"Scarf\",\"Skirt\",\"Face\",\"Left-arm\",\"Right-arm\",\"Left-leg\",\"Right-leg\",\"Left-shoe\",\"Right-shoe\"]\n",
        "\n",
        "\n",
        "  for i in range(len(truthMasks)):\n",
        "    anns = gt_to_anns_of_label_mask(truthMasks[i])\n",
        "    for ann in anns:\n",
        "      mask = masks[i]\n",
        "      image = images[i]\n",
        "      label =  label_name[ann['label']]\n",
        "      print(label_name[ann['label']])\n",
        "      # Get Corresponding gt Mask, generated Mask for Evaluation\n",
        "      print(mask[label])\n",
        "      iou = IOU(mask[label],ann['segmentation'])\n",
        "      print(iou)\n",
        "      result_image = get_masked_image(image, mask[label])\n",
        "      # display(result_image)\n",
        "      pixacc = pixAcc(mask[label],ann['segmentation'])\n",
        "      result = {\n",
        "          \"iou\": iou,\n",
        "          \"pixacc\": pixacc,\n",
        "          \"label_num\": ann[\"label\"],\n",
        "          \"label_name\": label,\n",
        "          \"masked_img\": result_image,\n",
        "          \"pred_mask\": mask[label],\n",
        "          \"gt_mask\": ann\n",
        "      }\n",
        "      print(\"iou:{}, pixacc:{}, label num:{}, label_name:{}\".format(iou, pixacc, ann[\"label\"], label))\n",
        "      results.append(result)\n",
        "\n",
        "  return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pw8pfG8RajZK"
      },
      "source": [
        "# MAIN TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "MFLt-puoTLli",
        "outputId": "0ded4861-b009-420c-a3a4-08d12ef8ffae"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Inference <span style=\"color: #f42670; text-decoration-color: #f42670\">━</span><span style=\"color: #e6276c; text-decoration-color: #e6276c\">━</span><span style=\"color: #d12a66; text-decoration-color: #d12a66\">━</span><span style=\"color: #b72c5e; text-decoration-color: #b72c5e\">━</span><span style=\"color: #993056; text-decoration-color: #993056\">━</span><span style=\"color: #7b334d; text-decoration-color: #7b334d\">━</span><span style=\"color: #613545; text-decoration-color: #613545\">━</span><span style=\"color: #4c383f; text-decoration-color: #4c383f\">━</span><span style=\"color: #3e393b; text-decoration-color: #3e393b\">━</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━</span><span style=\"color: #3e393b; text-decoration-color: #3e393b\">━</span><span style=\"color: #4c383f; text-decoration-color: #4c383f\">━</span><span style=\"color: #613545; text-decoration-color: #613545\">━</span><span style=\"color: #7b334d; text-decoration-color: #7b334d\">━</span><span style=\"color: #993056; text-decoration-color: #993056\">━</span><span style=\"color: #b72c5e; text-decoration-color: #b72c5e\">━</span><span style=\"color: #d12a66; text-decoration-color: #d12a66\">━</span><span style=\"color: #e6276c; text-decoration-color: #e6276c\">━</span><span style=\"color: #f42670; text-decoration-color: #f42670\">━</span><span style=\"color: #f92672; text-decoration-color: #f92672\">━</span><span style=\"color: #f42670; text-decoration-color: #f42670\">━</span><span style=\"color: #e6276c; text-decoration-color: #e6276c\">━</span><span style=\"color: #d12a66; text-decoration-color: #d12a66\">━</span><span style=\"color: #b72c5e; text-decoration-color: #b72c5e\">━</span><span style=\"color: #993056; text-decoration-color: #993056\">━</span><span style=\"color: #7b334d; text-decoration-color: #7b334d\">━</span><span style=\"color: #613545; text-decoration-color: #613545\">━</span><span style=\"color: #4c383f; text-decoration-color: #4c383f\">━</span><span style=\"color: #3e393b; text-decoration-color: #3e393b\">━</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━</span><span style=\"color: #3e393b; text-decoration-color: #3e393b\">━</span><span style=\"color: #4c383f; text-decoration-color: #4c383f\">━</span><span style=\"color: #613545; text-decoration-color: #613545\">━</span><span style=\"color: #7b334d; text-decoration-color: #7b334d\">━</span><span style=\"color: #993056; text-decoration-color: #993056\">━</span><span style=\"color: #b72c5e; text-decoration-color: #b72c5e\">━</span><span style=\"color: #d12a66; text-decoration-color: #d12a66\">━</span><span style=\"color: #e6276c; text-decoration-color: #e6276c\">━</span><span style=\"color: #f42670; text-decoration-color: #f42670\">━</span><span style=\"color: #f92672; text-decoration-color: #f92672\">━</span>  <span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "Inference \u001b[38;2;244;38;112m━\u001b[0m\u001b[38;2;230;39;108m━\u001b[0m\u001b[38;2;209;42;102m━\u001b[0m\u001b[38;2;183;44;94m━\u001b[0m\u001b[38;2;153;48;86m━\u001b[0m\u001b[38;2;123;51;77m━\u001b[0m\u001b[38;2;97;53;69m━\u001b[0m\u001b[38;2;76;56;63m━\u001b[0m\u001b[38;2;62;57;59m━\u001b[0m\u001b[38;2;58;58;58m━\u001b[0m\u001b[38;2;62;57;59m━\u001b[0m\u001b[38;2;76;56;63m━\u001b[0m\u001b[38;2;97;53;69m━\u001b[0m\u001b[38;2;123;51;77m━\u001b[0m\u001b[38;2;153;48;86m━\u001b[0m\u001b[38;2;183;44;94m━\u001b[0m\u001b[38;2;209;42;102m━\u001b[0m\u001b[38;2;230;39;108m━\u001b[0m\u001b[38;2;244;38;112m━\u001b[0m\u001b[38;2;249;38;114m━\u001b[0m\u001b[38;2;244;38;112m━\u001b[0m\u001b[38;2;230;39;108m━\u001b[0m\u001b[38;2;209;42;102m━\u001b[0m\u001b[38;2;183;44;94m━\u001b[0m\u001b[38;2;153;48;86m━\u001b[0m\u001b[38;2;123;51;77m━\u001b[0m\u001b[38;2;97;53;69m━\u001b[0m\u001b[38;2;76;56;63m━\u001b[0m\u001b[38;2;62;57;59m━\u001b[0m\u001b[38;2;58;58;58m━\u001b[0m\u001b[38;2;62;57;59m━\u001b[0m\u001b[38;2;76;56;63m━\u001b[0m\u001b[38;2;97;53;69m━\u001b[0m\u001b[38;2;123;51;77m━\u001b[0m\u001b[38;2;153;48;86m━\u001b[0m\u001b[38;2;183;44;94m━\u001b[0m\u001b[38;2;209;42;102m━\u001b[0m\u001b[38;2;230;39;108m━\u001b[0m\u001b[38;2;244;38;112m━\u001b[0m\u001b[38;2;249;38;114m━\u001b[0m  \u001b[36m \u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.5774534940719604, 0.5457377433776855, 0.49377748370170593, 0.48256364464759827, 0.4353873133659363, 0.4193853735923767, 0.415219783782959, 0.41242411732673645, 0.40560591220855713, 0.37351927161216736, 0.3728857636451721, 0.36547723412513733, 0.35771435499191284, 0.34557098150253296, 0.34536826610565186, 0.34191328287124634, 0.34107711911201477, 0.34000569581985474, 0.3380400538444519, 0.33262914419174194, 0.33130231499671936, 0.32460442185401917, 0.3213454484939575, 0.31908899545669556, 0.31776389479637146, 0.316559761762619, 0.3147299289703369, 0.3036641776561737, 0.302785724401474, 0.299793541431427, 0.2967337965965271, 0.2941032350063324, 0.29391801357269287, 0.2911301553249359, 0.29054415225982666, 0.29032090306282043, 0.2880649268627167, 0.2875385582447052, 0.28451406955718994, 0.2842302620410919, 0.28263169527053833, 0.2800970673561096, 0.2788475751876831, 0.27820315957069397, 0.27668091654777527, 0.27633270621299744, 0.274135023355484, 0.27279821038246155, 0.26987898349761963, 0.2691393494606018, 0.2675650715827942, 0.2668721377849579, 0.26632001996040344, 0.26572686433792114, 0.26509031653404236, 0.26264166831970215, 0.2620030641555786, 0.25926992297172546, 0.2584964632987976, 0.25657448172569275, 0.25654035806655884, 0.2522314190864563, 0.25145474076271057, 0.25070568919181824, 0.24589332938194275, 0.24384917318820953, 0.24082989990711212, 0.24035654962062836, 0.2389955073595047, 0.2378864288330078, 0.23737816512584686, 0.23645025491714478, 0.23405247926712036, 0.23300132155418396, 0.22966112196445465, 0.228745236992836, 0.22634868323802948, 0.22552622854709625, 0.22501209378242493, 0.22475332021713257, 0.22390611469745636, 0.22333396971225739, 0.22302059829235077, 0.22296884655952454, 0.22193479537963867, 0.22091346979141235, 0.22060316801071167, 0.22014081478118896, 0.21993643045425415, 0.21805180609226227, 0.21629856526851654, 0.21598850190639496, 0.2152920961380005, 0.21512046456336975, 0.2144080549478531, 0.21362552046775818, 0.21219240128993988, 0.2109561711549759, 0.21047157049179077, 0.2104647159576416]\n",
            "[9.153665542602539, 7.064716339111328, 32.0672607421875, 90.28614044189453]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "images, truthMasks, imagePaths = getImages(32)\n",
        "\n",
        "bodyPrompts = [\"Background\",\"Hat\",\"Hair\",\"Glove\", \"Sunglasses\",\"UpperClothes\",\"Dress\",\"Coat\",\"Socks\",\"Pants\", \"Jumpsuits\",\"Scarf\",\"Skirt\",\"Face\",\"Left-arm\",\"Right-arm\",\"Left-leg\",\"Right-leg\",\"Left-shoe\",\"Right-shoe\"]\n",
        "# bodyPrompts = [\"Socks\", \"Hat\"]\n",
        "\n",
        "boundingBoxes, preds, masks = getFinalPredictions(images, imagePaths, bodyPrompts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhC3Sk6hgbth"
      },
      "source": [
        "# SAVING DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HJ7bVm3TLli"
      },
      "outputs": [],
      "source": [
        "def compute_pix_acc(predicted, target):\n",
        "\n",
        "    assert predicted.shape == target.shape\n",
        "    assert len(predicted.shape) == 2\n",
        "    return (predicted == target).mean()\n",
        "\n",
        "def compute_IOU(predicted, target):\n",
        "\n",
        "    assert predicted.shape == target.shape\n",
        "\n",
        "    assert len(predicted.shape) == 2\n",
        "    intersection = np.logical_and(target, predicted).sum()\n",
        "    union = np.logical_or(target, predicted).sum()\n",
        "    assert union > 0\n",
        "    return intersection / union\n",
        "def compute_metric(name, masks, label):\n",
        "    \"\"\" name: data_id\n",
        "        mask: { label_id: numpy.ndarray(shape=(H, W)) }\n",
        "        label: np.ndarray(shape=(H, W)) --> numbers from 0 to 19\n",
        "    \"\"\"\n",
        "    pix_acc_metric = { \"name\": name }\n",
        "    iou_metric = { \"name\": name }\n",
        "    empty = np.zeros_like(label)\n",
        "    LABELS = [\"Background\",\"Hat\",\"Hair\",\"Glove\",\n",
        "        \"Sunglasses\",\"UpperClothes\",\"Dress\",\"Coat\",\"Socks\",\"Pants\",\n",
        "        \"Jumpsuits\",\"Scarf\",\"Skirt\",\"Face\",\"Left-arm\",\"Right-arm\",\"Left-leg\",\"Right-leg\",\"Left-shoe\",\"Right-shoe\"]\n",
        "\n",
        "    for i, label_name in enumerate(LABELS):\n",
        "        mask_i = masks.get(label_name, empty)\n",
        "        label_i = (label == i)\n",
        "        if label_i.sum() == 0:\n",
        "            # pandas dataframe automatically skips nan\n",
        "            # when computing .count() and .mean()\n",
        "            iou_metric[label_name] = np.nan\n",
        "            pix_acc_metric[label_name] = np.nan\n",
        "        else:\n",
        "            iou_metric[label_name] = compute_IOU(mask_i, label_i)\n",
        "            pix_acc_metric[label_name] = compute_pix_acc(mask_i, label_i)\n",
        "\n",
        "    return iou_metric, pix_acc_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9o_V-NyTLli"
      },
      "outputs": [],
      "source": [
        "def getImages2(num):\n",
        "    images = []\n",
        "    truthMasks = []\n",
        "    data_list2 = []\n",
        "    root = \"\"\n",
        "    # root = \"/content/drive/MyDrive/CSCI567/segment-anything/datasets/people_poses/\"\n",
        "    textFile = \"val_id.short.txt\"\n",
        "    imageFile = \"val_images/\"\n",
        "    segmentationFile = \"val_segmentations/\"\n",
        "    with open(os.path.join(root, f\"val_id.short.txt\"), 'r') as lf:\n",
        "        data_list = [ s.strip() for s in lf.readlines() ]\n",
        "\n",
        "    num_valid_case, sum_miou, sum_pixAcc = 0,0,0\n",
        "    try:\n",
        "        for data_name in (pbar := tqdm(data_list[:num])):\n",
        "            img_path = root + imageFile + data_name + '.jpg'\n",
        "            seg_path = root +  segmentationFile+ data_name + '.png'\n",
        "\n",
        "            # Read Image and Ground truth mask\n",
        "            img = copy.deepcopy(Image.open(img_path))\n",
        "\n",
        "            # display(img)\n",
        "            if img is None:\n",
        "                print(\"\\nimage is None\", data_name)\n",
        "                continue\n",
        "            else:\n",
        "                data_list2.append(data_name)\n",
        "                images.append(img)\n",
        "            mask_gt = cv2.imread(seg_path)\n",
        "            if mask_gt is None:\n",
        "                print(\"\\nmask_gt is None\", data_name)\n",
        "                continue\n",
        "            else:\n",
        "                truthMasks.append(mask_gt)\n",
        "    except Exception as e:\n",
        "        print(\"ERROR\")\n",
        "        print(e)\n",
        "    return images, truthMasks, data_list2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwhRO0JRTLli"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# results = evaluate(images, masks, truthMasks)\n",
        "from PIL import Image, Image, ImageDraw\n",
        "from torch.jit import Error\n",
        "import pandas as pd\n",
        "# numpy metrics\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "# root = \"/content/drive/MyDrive/CSCI567/segment-anything/datasets/people_poses/\"\n",
        "root = \"\"\n",
        "prompt = \"The object of \"\n",
        "\n",
        "# try:\n",
        "miou_table = []\n",
        "pix_acc_table = []\n",
        "images, truthMasks, data_list = getImages2(32)\n",
        "print(len(truthMasks))\n",
        "print(len(images))\n",
        "print(len(data_list))\n",
        "\n",
        "result_table = {}\n",
        "i = 0\n",
        "try:\n",
        "  for data_name in data_list:\n",
        "\n",
        "    anns = gt_to_anns_of_label_mask(truthMasks[i])\n",
        "    miou, pix_acc = compute_metric(data_name, masks[i], truthMasks[i][:,:,0])\n",
        "    miou_table.append(miou)\n",
        "    pix_acc_table.append(pix_acc)\n",
        "    predict_anns = []\n",
        "    for ann in anns:\n",
        "\n",
        "      ## Get Label Index with Highest Score\n",
        "      predict_anns.append({\n",
        "          'segmentation': masks[i],\n",
        "          'label': ann['label'],\n",
        "          'gt': ann['segmentation'],\n",
        "      })\n",
        "    i += 1\n",
        "    result_table[data_name] = predict_anns\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "  print(miou_table)\n",
        "  print(pix_acc_table)\n",
        "\n",
        "miou_table = pd.DataFrame(miou_table, columns=miou_table[0].keys()).set_index('name')\n",
        "miou_table.to_csv('/Users/arianasokolov/Desktop/ClipSegSam/CSVs/miou.csv')\n",
        "np.save('vis_cliseg_sam_32.npy', result_table)\n",
        "\n",
        "pix_acc_table = pd.DataFrame(pix_acc_table, columns=pix_acc_table[0].keys()).set_index('name')\n",
        "pix_acc_table.to_csv('/Users/arianasokolov/Desktop/ClipSegSam/CSVs/pix_acc.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rep8ON9ggfk8"
      },
      "source": [
        "# VISUALIZATION\n",
        "\n",
        "EVERYTHING BELOW THIS SECTION IS JUST TO TEST THINGS AND DOESN'T MATTER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eemhzXM4TLli"
      },
      "outputs": [],
      "source": [
        "print(result_table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "os0-rQ2KTLli"
      },
      "outputs": [],
      "source": [
        "def show_mask(mask, ax, random_color=False):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)\n",
        "def get_mask(mask,color=None):\n",
        "    if color is None:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    # else:\n",
        "    #     color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    return mask_image\n",
        "\n",
        "def show_box(box, ax):\n",
        "    x0, y0 = box[0], box[1]\n",
        "    w, h = box[2] - box[0], box[3] - box[1]\n",
        "    return plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ug7mAq6uTLli"
      },
      "outputs": [],
      "source": [
        "prompts = bodyPrompts[:2]\n",
        "last = 1\n",
        "# *****\n",
        "# Setup plot\n",
        "_, ax = plt.subplots(len(boundingBoxes[:last]), len(prompts) + 1, figsize=(3*(len(prompts) + 1), 4))\n",
        "[a.axis('off') for a in ax.flatten()]\n",
        "\n",
        "for i in range(len(boundingBoxes[:last])):\n",
        "  # Show padded image\n",
        "  print(imagePaths[i])\n",
        "  ax[0].imshow(images[i])\n",
        "\n",
        "  for j in range(len(prompts)):\n",
        "    # Show mask\n",
        "    print(prompts[j])\n",
        "    print(preds[i])\n",
        "    # predictionImage = processPredictionImage(preds[i][prompts[j]], images[i])\n",
        "    # ax[i][j+1].imshow(predictionImage)\n",
        "    # ax[i][j+1].imshow(get_mask(masks[0][\"Socks\"]))\n",
        "\n",
        "    # Show bounding box\n",
        "    print(boundingBoxes[i][prompts[j]])\n",
        "    ax[0].add_patch(show_box(boundingBoxes[i][prompts[j]], plt.gca()))\n",
        "\n",
        "\n",
        "    # Show prompt\n",
        "    # ax[i][j+1].text(0, -15, prompts[j])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcHmNt2-TLli"
      },
      "outputs": [],
      "source": [
        "input_box = np.array(boundingBoxes[0][\"Socks\"])\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(images[0])\n",
        "show_mask(masks[0][\"Socks\"], plt.gca())\n",
        "# plt.gca().add_patch(show_box(input_box, plt.gca()))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-coJ3rY4TLli"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "rhC3Sk6hgbth"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "c1330ff811d49988c6bd3f6d30257d79d8234a7d606d1a91e87e277837a80053"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}