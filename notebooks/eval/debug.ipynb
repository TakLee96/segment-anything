{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1619dc1-be6b-43e0-8bbd-39c62eda76d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jiaha\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 423, 187)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "def gt_to_anns(mask_gt):\n",
    "    labels = np.unique(mask_gt)\n",
    "    anns = []\n",
    "    for label in labels:\n",
    "        # skip background\n",
    "        if label == 0:\n",
    "            continue\n",
    "        mask = np.all(mask_gt == label, axis=-1)\n",
    "        # 1 ramdon point from mask\n",
    "        num_point = 1\n",
    "        indices = np.argwhere(mask)\n",
    "        # swap x y\n",
    "        indices[:,[1,0]] = indices[:,[0,1]]\n",
    "        # sample on random point\n",
    "        point = np.asarray(indices[np.random.randint(indices.shape[0], size=num_point)])\n",
    "        # all point for test\n",
    "        points = np.asarray(indices)\n",
    "        anns.append({\n",
    "            'area': np.sum(mask),\n",
    "            'segmentation': mask,\n",
    "            'label': label,\n",
    "            'random_point_1':point,\n",
    "            'points':points,\n",
    "        })\n",
    "    return anns\n",
    "\n",
    "sam_checkpoint = \"../../sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "device = \"cuda:0\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam = sam.to(device)\n",
    "predictor = SamPredictor(sam)\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "root = \"../../datasets/people_poses/\"\n",
    "data_name = '100034_483681'\n",
    "image = cv2.imread(root + 'val_images/' + data_name + '.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "mask_gt = cv2.imread(root + 'val_segmentations/' + data_name + '.png')\n",
    "anns = gt_to_anns(mask_gt)\n",
    "predictor.set_image(image)\n",
    "\n",
    "target = anns[2]\n",
    "assert target['label'] == 6\n",
    "\n",
    "masks, _, _ = predictor.predict(\n",
    "    point_coords=np.array([[107, 252]], dtype=np.int64),\n",
    "    point_labels=np.array([1]),\n",
    "    multimask_output=False,\n",
    ")\n",
    "print(masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96f47b80-6ee6-4fc8-90bd-8f626336fc3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cuda', index=0),\n",
       " torch.Size([1, 256, 64, 64]),\n",
       " None,\n",
       " None,\n",
       " (1024, 453),\n",
       " True,\n",
       " None,\n",
       " None,\n",
       " (423, 187),\n",
       " <segment_anything.utils.transforms.ResizeLongestSide at 0x1628c38fee0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.device, predictor.features.shape, predictor.input_h, predictor.input_w, predictor.input_size, predictor.is_image_set, predictor.orig_h, predictor.orig_w, predictor.original_size, predictor.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42daaa7a-4830-4a1c-8a6d-994f2473ef96",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = predictor.features.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06d34824-f809-470f-9933-c3571efe07c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(423, 187)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target['segmentation'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfa9697c-baf0-4682-a1cc-fd722e04f72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.991390753593507"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(masks[0] == target['segmentation']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b851ee31-94f4-42aa-a995-8e19950497fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('../../datasets/people_poses/val_embeds/' + data_name + '.npz')\n",
    "embed = data['embed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a17b4126-5f57-4b9c-8831-693bedcb11c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 423, 187)\n"
     ]
    }
   ],
   "source": [
    "def set_embedding(predictor, embed, label):\n",
    "    predictor.is_image_set = True\n",
    "    predictor.features = torch.as_tensor(embed[None,]).to(device)\n",
    "    # image_batch, mask_batch, height, width\n",
    "    assert len(predictor.features.shape) == 4\n",
    "    predictor.original_size = label.shape\n",
    "    # TODO(jiahang): fix magic numbers\n",
    "    predictor.input_size = (1024, 1024)\n",
    "set_embedding(predictor, embed, target['segmentation'])\n",
    "masks2, _, _ = predictor.predict(\n",
    "    point_coords=np.array([[107, 252]], dtype=np.int64),\n",
    "    point_labels=np.array([1]),\n",
    "    multimask_output=False,\n",
    ")\n",
    "print(masks2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82870777-9a90-4aa6-8125-615c62ec0fc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9115813959368403"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(masks2[0] == target['segmentation']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "937992e5-092a-44d8-9123-47f9d6773f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 453)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.transform.get_preprocess_shape(423, 187, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a05d614-5a54-4cf9-9ca3-42a9ef05ddda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
