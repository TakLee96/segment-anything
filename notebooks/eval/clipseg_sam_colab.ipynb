{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "bWZmgAyOpfWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfN3e_6aUnDe"
      },
      "source": [
        "# Using CLIPSeg with Hugging Face Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JE8CVZ86W2Ru"
      },
      "source": [
        "Using Hugging Face Transformers, you can easily download and run a pre-trained CLIPSeg model on your images. Let’s start by installing transformers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYigMTweWttc"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10YAoBoIW3zn"
      },
      "source": [
        "To download the model, simply instantiate it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCdgxg4SXg_r"
      },
      "outputs": [],
      "source": [
        "from transformers import CLIPSegProcessor, CLIPSegForImageSegmentation\n",
        "\n",
        "processor = CLIPSegProcessor.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n",
        "model = CLIPSegForImageSegmentation.from_pretrained(\"CIDAS/clipseg-rd64-refined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjnCAKnaU04t"
      },
      "source": [
        "## Text prompting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qc4IuietEqr1"
      },
      "source": [
        "Let’s start by defining some text categories we want to segment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVWkP-j0W8WO"
      },
      "source": [
        "Now that we have our inputs, we can process them and input them to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfCjKfgtW1_M"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "def getPredictions(promptValue, image):\n",
        "  prompts = [promptValue]\n",
        "  inputs = processor(text=prompts, images=[image] * len(prompts), padding=\"max_length\", return_tensors=\"pt\")\n",
        "  # predict\n",
        "  with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "  return outputs.logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73iz0q_Be3u4"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "def make_square_by_padding(img, fill_color=(0, 0, 0)):\n",
        "    # img = Image.open(requests.get(url, stream=True).raw)\n",
        "    width, height = img.size\n",
        "\n",
        "    # Determine the size for the square\n",
        "    new_size = max(width, height)\n",
        "\n",
        "    # Create a new image with the desired size and fill color\n",
        "    new_img = Image.new(\"RGB\", (new_size, new_size), fill_color)\n",
        "\n",
        "    # Paste the original image onto the center of the new image\n",
        "    new_img.paste(img, ((new_size - width) // 2, (new_size - height) // 2))\n",
        "\n",
        "    return new_img\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3KdTxyFe3u4"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.ops import masks_to_boxes\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image, Image, ImageDraw\n",
        "import numpy as np\n",
        "# ***\n",
        "def processPredictionImage(pred, img):\n",
        "    mask = torch.sigmoid(torch.reshape(torch.tensor(pred), (352, 352)))\n",
        "    # ax[i+1].imshow(mask)\n",
        "\n",
        "    processed_tensor = torch.sigmoid(torch.reshape(torch.tensor(pred), (352, 352)))\n",
        "    image_np = (processed_tensor.detach().numpy() * 255).astype(np.uint8)\n",
        "\n",
        "    _, binary_image = cv2.threshold(image_np, 127, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    desired_width, desired_height = img.size\n",
        "    resized_image = cv2.resize(binary_image, (max(desired_width, desired_height),  max(desired_width, desired_height)), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    # Get dimensions of the binary image\n",
        "    height, width = resized_image.shape[:2]\n",
        "\n",
        "    # Check if the desired crop size is smaller than the original image size\n",
        "    if desired_width <= width and desired_height <= height:\n",
        "        # Calculate the top-left corner of the crop\n",
        "        x = width // 2 - desired_width // 2\n",
        "        y = height // 2 - desired_height // 2\n",
        "\n",
        "        # Crop the image\n",
        "        cropped_image = resized_image[y:y+desired_height, x:x+desired_width]\n",
        "        return cropped_image\n",
        "\n",
        "    else:\n",
        "        print(\"failed to crop image\")\n",
        "        return resized_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APLT-SU3e3u5"
      },
      "outputs": [],
      "source": [
        "# ***\n",
        "def getBoundingBox(predictionImage):\n",
        "    contours, _ = cv2.findContours(predictionImage, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    if contours:\n",
        "        list_of_pts = []\n",
        "        for ctr in contours:\n",
        "            list_of_pts += [pt[0] for pt in ctr]\n",
        "        ctr = np.array(list_of_pts).reshape((-1,1,2)).astype(np.int32)\n",
        "        # largest_contour = max(cv2.convexHull(ctr), key=cv2.contourArea)\n",
        "        x, y, w, h = cv2.boundingRect(cv2.convexHull(ctr))\n",
        "\n",
        "        print(f\"Bounding Box: x={x}, y={y}, width={(w)}, height={(h)}\")\n",
        "        return [x, y, (w + x), (h + y)]\n",
        "\n",
        "    else:\n",
        "        print(\"No contours found\")\n",
        "        return [0, 0, 0, 0]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3_jVSboe3u5"
      },
      "source": [
        "# SAM Integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDyugZYde3u5"
      },
      "outputs": [],
      "source": [
        "!pip install torch opencv-python Pillow\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install -q 'git+https://github.com/facebookresearch/segment-anything.git'\n",
        "!pip install -q jupyter_bbox_widget roboflow dataclasses-json supervision\n",
        "\n",
        "!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T062niiIe3u5"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "\n",
        "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
        "model_type = \"vit_h\"\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "sam.to(device=device)\n",
        "\n",
        "predictor = SamPredictor(sam)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDtNsE80e3u5"
      },
      "outputs": [],
      "source": [
        "def getSAMPreditction(image, box):\n",
        "\n",
        "    image_np = np.array(image)\n",
        "    image2 = cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB)\n",
        "    predictor.set_image(image2)\n",
        "\n",
        "    input_box = np.array(box)\n",
        "    if(input_box[0] == 0 and input_box[1] == 0 and input_box[2] == 0 and input_box[3] == 0):\n",
        "        masks, _, _ = predictor.predict(\n",
        "            point_coords=None,\n",
        "            point_labels=None,\n",
        "            box=None,\n",
        "            multimask_output=False,\n",
        "        )\n",
        "    else:\n",
        "        masks, _, _ = predictor.predict(\n",
        "            point_coords=None,\n",
        "            point_labels=None,\n",
        "            box=input_box[None, :],\n",
        "            multimask_output=False,\n",
        "        )\n",
        "    return masks[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4yIcUzde3u6"
      },
      "outputs": [],
      "source": [
        "# ****\n",
        "# numpy version\n",
        "def pixAcc(predicted, target):\n",
        "    same = (predicted == target).sum()\n",
        "    w, h = target.shape\n",
        "    print(\"Target seg shape: {}, Predicted seg shape: {}, #Same pixels: {}\".format(target.shape, predicted.shape, same))\n",
        "    return same / (w * h)\n",
        "\n",
        "# input: bool matrix\n",
        "def IOU(predicted , target):\n",
        "    intersection = np.logical_and(target, predicted).sum()\n",
        "    union = np.logical_or(target, predicted).sum()\n",
        "    if union == 0:\n",
        "        iou_score = 0\n",
        "    else :\n",
        "        iou_score = intersection / union\n",
        "    return iou_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mW0bL6VCe3u6"
      },
      "outputs": [],
      "source": [
        "# *****\n",
        "def gt_to_anns_of_label_mask(mask_gt):\n",
        "  labels = np.unique(mask_gt)\n",
        "  anns = []\n",
        "  for label in labels:\n",
        "    # skip background\n",
        "      if label == 0:\n",
        "          continue\n",
        "      mask = np.all(mask_gt == label, axis=-1)\n",
        "      anns.append({\n",
        "        'area': np.sum(mask),\n",
        "        'segmentation': mask,\n",
        "        'label': label,\n",
        "      })\n",
        "  return anns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HOk9oELe3u6"
      },
      "source": [
        "# Final Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvSEdqao6WcA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# ***\n",
        "\n",
        "def getFinalPredictions(images, promptVals):\n",
        "  preds = []\n",
        "  boundingBoxes = []\n",
        "  masks = []\n",
        "  emptyPrompt = \"\"\n",
        "  for img in images:\n",
        "    imgResults = {}\n",
        "    imageBoundingBoxes = {}\n",
        "    imageMasks = {}\n",
        "    img_with_border = make_square_by_padding(img)\n",
        "\n",
        "    for prompt in promptVals:\n",
        "      prediction = getPredictions(prompt, img_with_border)\n",
        "      imgResults[prompt] = prediction\n",
        "      predictionImage = processPredictionImage(prediction, img)\n",
        "\n",
        "      bbox = getBoundingBox(predictionImage)\n",
        "      imageBoundingBoxes[prompt] = bbox\n",
        "\n",
        "      if(bbox[0] == 0 and bbox[1] == 0 and bbox[2] == 0 and bbox[3] == 0 and emptyPrompt == \"\"):\n",
        "        print(\"Here\")\n",
        "        mask = getSAMPreditction(img, bbox)\n",
        "        imageMasks[prompt] = mask\n",
        "        emptyPrompt = prompt\n",
        "      elif(bbox[0] == 0 and bbox[1] == 0 and bbox[2] == 0 and bbox[3] == 0):\n",
        "        print(\"Here\")\n",
        "        imageMasks[prompt] = imageMasks[emptyPrompt]\n",
        "      else:\n",
        "        mask = getSAMPreditction(img, bbox)\n",
        "        imageMasks[prompt] = mask\n",
        "\n",
        "    emptyPrompt = \"\"\n",
        "    preds.append(imgResults)\n",
        "    boundingBoxes.append(imageBoundingBoxes)\n",
        "    masks.append(imageMasks)\n",
        "\n",
        "  preds = np.array(preds)\n",
        "  boundingBoxes = np.array(boundingBoxes)\n",
        "  return boundingBoxes, preds, masks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lee0TxXue3u6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "# ***\n",
        "def getImages(num):\n",
        "    images = []\n",
        "    truthMasks = []\n",
        "    data_list = []\n",
        "    root = \"\"\n",
        "    # root = \"/content/drive/MyDrive/CSCI567/segment-anything/datasets/people_poses/\"\n",
        "    textFile = \"val_id.txt\"\n",
        "    imageFile = \"val_images/\"\n",
        "    segmentationFile = \"val_segmentations/\"\n",
        "    with open(os.path.join(root, f\"val_id.txt\"), 'r') as lf:\n",
        "        data_list = [ s.strip() for s in lf.readlines() ]\n",
        "\n",
        "    num_valid_case, sum_miou, sum_pixAcc = 0,0,0\n",
        "    try:\n",
        "        for data_name in (pbar := tqdm(data_list[:num])):\n",
        "            img_path = root + imageFile + data_name + '.jpg'\n",
        "            seg_path = root +  segmentationFile+ data_name + '.png'\n",
        "\n",
        "            # Read Image and Ground truth mask\n",
        "            img = copy.deepcopy(Image.open(img_path))\n",
        "\n",
        "            # display(img)\n",
        "            if img is None:\n",
        "                print(\"\\nimage is None\", data_name)\n",
        "                continue\n",
        "            else:\n",
        "                images.append(img)\n",
        "            mask_gt = cv2.imread(seg_path)\n",
        "            if mask_gt is None:\n",
        "                print(\"\\nmask_gt is None\", data_name)\n",
        "                continue\n",
        "            else:\n",
        "                truthMasks.append(mask_gt)\n",
        "    except Exception as e:\n",
        "        print(\"ERROR\")\n",
        "        print(e)\n",
        "    return images, truthMasks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXZiEHFle3u6"
      },
      "outputs": [],
      "source": [
        "def get_masked_image(original_image, segmentation):\n",
        "  # Visualize\n",
        "\n",
        "  overlay_image = Image.new('RGBA', original_image.size, (0, 0, 0, 0))\n",
        "  overlay_color = (255, 0, 0, 200)\n",
        "\n",
        "  draw = ImageDraw.Draw(overlay_image)\n",
        "  segmentation_mask_image = Image.fromarray(segmentation.astype('uint8') * 255)\n",
        "  draw.bitmap((0, 0), segmentation_mask_image, fill=overlay_color)\n",
        "\n",
        "  return Image.alpha_composite(original_image.convert('RGBA'), overlay_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4m6-MUd1e3u6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# ***\n",
        "# TODO replace for body pose images\n",
        "images, truthMasks = getImages(10000)\n",
        "\n",
        "bodyPrompts = [\"Background\",\"Hat\",\"Hair\",\"Glove\", \"Sunglasses\",\"UpperClothes\",\"Dress\",\"Coat\",\"Socks\",\"Pants\", \"Jumpsuits\",\"Scarf\",\"Skirt\",\"Face\",\"Left-arm\",\"Right-arm\",\"Left-leg\",\"Right-leg\",\"Left-shoe\",\"Right-shoe\"]\n",
        "# bodyPrompts = [\"Socks\", \"Hat\"]\n",
        "\n",
        "boundingBoxes, preds, masks = getFinalPredictions(images, bodyPrompts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUsA564_e3u6"
      },
      "outputs": [],
      "source": [
        "\n",
        "def evaluate(images, masks, truthMasks):\n",
        "  # Get confidence scores for each masks generated by SAM for\n",
        "  # each object label existing in the given image\n",
        "\n",
        "  results = []\n",
        "  label_name = [\"Background\",\"Hat\",\"Hair\",\"Glove\",\n",
        "        \"Sunglasses\",\"UpperClothes\",\"Dress\",\"Coat\",\"Socks\",\"Pants\",\n",
        "        \"Jumpsuits\",\"Scarf\",\"Skirt\",\"Face\",\"Left-arm\",\"Right-arm\",\"Left-leg\",\"Right-leg\",\"Left-shoe\",\"Right-shoe\"]\n",
        "\n",
        "\n",
        "  for i in range(len(truthMasks)):\n",
        "    anns = gt_to_anns_of_label_mask(truthMasks[i])\n",
        "    for ann in anns:\n",
        "      mask = masks[i]\n",
        "      image = images[i]\n",
        "      label =  label_name[ann['label']]\n",
        "      print(label_name[ann['label']])\n",
        "      # Get Corresponding gt Mask, generated Mask for Evaluation\n",
        "      print(mask[label])\n",
        "      iou = IOU(mask[label],ann['segmentation'])\n",
        "      print(iou)\n",
        "      result_image = get_masked_image(image, mask[label])\n",
        "      # display(result_image)\n",
        "      pixacc = pixAcc(mask[label],ann['segmentation'])\n",
        "      result = {\n",
        "          \"iou\": iou,\n",
        "          \"pixacc\": pixacc,\n",
        "          \"label_num\": ann[\"label\"],\n",
        "          \"label_name\": label,\n",
        "          \"masked_img\": result_image,\n",
        "          \"pred_mask\": mask[label],\n",
        "          \"gt_mask\": ann\n",
        "      }\n",
        "      print(\"iou:{}, pixacc:{}, label num:{}, label_name:{}\".format(iou, pixacc, ann[\"label\"], label))\n",
        "      results.append(result)\n",
        "\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLjQ77NSe3u6"
      },
      "outputs": [],
      "source": [
        "def compute_pix_acc(predicted, target):\n",
        "\n",
        "    assert predicted.shape == target.shape\n",
        "    assert len(predicted.shape) == 2\n",
        "    return (predicted == target).mean()\n",
        "\n",
        "def compute_IOU(predicted, target):\n",
        "\n",
        "    assert predicted.shape == target.shape\n",
        "\n",
        "    assert len(predicted.shape) == 2\n",
        "    intersection = np.logical_and(target, predicted).sum()\n",
        "    union = np.logical_or(target, predicted).sum()\n",
        "    assert union > 0\n",
        "    return intersection / union\n",
        "def compute_metric(name, masks, label):\n",
        "    \"\"\" name: data_id\n",
        "        mask: { label_id: numpy.ndarray(shape=(H, W)) }\n",
        "        label: np.ndarray(shape=(H, W)) --> numbers from 0 to 19\n",
        "    \"\"\"\n",
        "    pix_acc_metric = { \"name\": name }\n",
        "    iou_metric = { \"name\": name }\n",
        "    empty = np.zeros_like(label)\n",
        "    LABELS = [\"Background\",\"Hat\",\"Hair\",\"Glove\",\n",
        "        \"Sunglasses\",\"UpperClothes\",\"Dress\",\"Coat\",\"Socks\",\"Pants\",\n",
        "        \"Jumpsuits\",\"Scarf\",\"Skirt\",\"Face\",\"Left-arm\",\"Right-arm\",\"Left-leg\",\"Right-leg\",\"Left-shoe\",\"Right-shoe\"]\n",
        "\n",
        "    for i, label_name in enumerate(LABELS):\n",
        "        mask_i = masks.get(label_name, empty)\n",
        "        label_i = (label == i)\n",
        "        if label_i.sum() == 0:\n",
        "            # pandas dataframe automatically skips nan\n",
        "            # when computing .count() and .mean()\n",
        "            iou_metric[label_name] = np.nan\n",
        "            pix_acc_metric[label_name] = np.nan\n",
        "        else:\n",
        "            iou_metric[label_name] = compute_IOU(mask_i, label_i)\n",
        "            pix_acc_metric[label_name] = compute_pix_acc(mask_i, label_i)\n",
        "\n",
        "    return iou_metric, pix_acc_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZ9fny60e3u7"
      },
      "outputs": [],
      "source": [
        "def getImages2(num):\n",
        "    images = []\n",
        "    truthMasks = []\n",
        "    data_list2 = []\n",
        "    root = \"\"\n",
        "    # root = \"/content/drive/MyDrive/CSCI567/segment-anything/datasets/people_poses/\"\n",
        "    textFile = \"val_id.txt\"\n",
        "    imageFile = \"val_images/\"\n",
        "    segmentationFile = \"val_segmentations/\"\n",
        "    with open(os.path.join(root, f\"val_id.txt\"), 'r') as lf:\n",
        "        data_list = [ s.strip() for s in lf.readlines() ]\n",
        "\n",
        "    num_valid_case, sum_miou, sum_pixAcc = 0,0,0\n",
        "    try:\n",
        "        for data_name in (pbar := tqdm(data_list[:num])):\n",
        "            img_path = root + imageFile + data_name + '.jpg'\n",
        "            seg_path = root +  segmentationFile+ data_name + '.png'\n",
        "\n",
        "            # Read Image and Ground truth mask\n",
        "            img = copy.deepcopy(Image.open(img_path))\n",
        "\n",
        "            # display(img)\n",
        "            if img is None:\n",
        "                print(\"\\nimage is None\", data_name)\n",
        "                continue\n",
        "            else:\n",
        "                data_list2.append(data_name)\n",
        "                images.append(img)\n",
        "            mask_gt = cv2.imread(seg_path)\n",
        "            if mask_gt is None:\n",
        "                print(\"\\nmask_gt is None\", data_name)\n",
        "                continue\n",
        "            else:\n",
        "                truthMasks.append(mask_gt)\n",
        "    except Exception as e:\n",
        "        print(\"ERROR\")\n",
        "        print(e)\n",
        "    return images, truthMasks, data_list2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UT0iBgDwe3u7"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# results = evaluate(images, masks, truthMasks)\n",
        "\n",
        "from torch.jit import Error\n",
        "import pandas as pd\n",
        "# numpy metrics\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "# root = \"/content/drive/MyDrive/CSCI567/segment-anything/datasets/people_poses/\"\n",
        "root = \"\"\n",
        "prompt = \"The object of \"\n",
        "\n",
        "# try:\n",
        "miou_table = []\n",
        "pix_acc_table = []\n",
        "images, truthMasks, data_list = getImages2(10000)\n",
        "print(len(truthMasks))\n",
        "print(len(images))\n",
        "print(len(data_list))\n",
        "i = 0\n",
        "try:\n",
        "  for data_name in data_list:\n",
        "\n",
        "\n",
        "    miou, pix_acc = compute_metric(data_name, masks[i], truthMasks[i][:,:,0])\n",
        "    miou_table.append(miou)\n",
        "    pix_acc_table.append(pix_acc)\n",
        "    i += 1\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "  print(miou_table)\n",
        "  print(pix_acc_table)\n",
        "\n",
        "miou_table = pd.DataFrame(miou_table, columns=miou_table[0].keys()).set_index('name')\n",
        "miou_table.to_csv('miou.csv')\n",
        "\n",
        "pix_acc_table = pd.DataFrame(pix_acc_table, columns=pix_acc_table[0].keys()).set_index('name')\n",
        "pix_acc_table.to_csv('pix_acc.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYpV9474r-pS"
      },
      "source": [
        "Finally, let’s visualize the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DURP5C7Je3u7"
      },
      "outputs": [],
      "source": [
        "# def show_mask(mask, ax, random_color=False):\n",
        "#     if random_color:\n",
        "#         color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "#     else:\n",
        "#         color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "#     h, w = mask.shape[-2:]\n",
        "#     mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "#     ax.imshow(mask_image)\n",
        "\n",
        "# def show_box(box, ax):\n",
        "#     x0, y0 = box[0], box[1]\n",
        "#     w, h = box[2] - box[0], box[3] - box[1]\n",
        "#     return plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6eHine5e3u7"
      },
      "outputs": [],
      "source": [
        "# prompts = bodyPrompts[:3]\n",
        "# last = 2\n",
        "# # *****\n",
        "# # Setup plot\n",
        "# _, ax = plt.subplots(len(boundingBoxes[:last]), len(prompts) + 1, figsize=(3*(len(prompts) + 1), 4))\n",
        "# [a.axis('off') for a in ax.flatten()]\n",
        "\n",
        "# for i in range(len(boundingBoxes[:last])):\n",
        "#   # Show padded image\n",
        "#   ax[i][0].imshow(images[i])\n",
        "\n",
        "#   for j in range(len(prompts)):\n",
        "#     # Show mask\n",
        "#     predictionImage = processPredictionImage(preds[i][j], images[i])\n",
        "#     ax[i][j+1].imshow(predictionImage)\n",
        "\n",
        "#     # Show bounding box\n",
        "#     # x, y, w, h =  boundingBoxes[i][prompts[j]]\n",
        "#     # rect = patches.Rectangle((x , y), w, h, linewidth=1, edgecolor='r', facecolor='none')\n",
        "#     print(boundingBoxes[i][prompts[j]])\n",
        "#     ax[i][j+1].add_patch(show_box(boundingBoxes[i][prompts[j]], plt.gca()))\n",
        "\n",
        "#     # Show prompt\n",
        "#     ax[i][j+1].text(0, -15, prompts[j])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWLVVUcEe3u7"
      },
      "outputs": [],
      "source": [
        "# input_box = np.array(boundingBoxes[0][\"Socks\"])\n",
        "\n",
        "\n",
        "# plt.figure(figsize=(10, 10))\n",
        "# plt.imshow(images[0])\n",
        "# show_mask(masks[0][\"Socks\"], plt.gca())\n",
        "# plt.gca().add_patch(show_box(input_box, plt.gca()))\n",
        "# plt.axis('off')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Xx7Vpgxe3u7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "c1330ff811d49988c6bd3f6d30257d79d8234a7d606d1a91e87e277837a80053"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}