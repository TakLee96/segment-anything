{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfN3e_6aUnDe"
      },
      "source": [
        "# Using CLIPSeg with Hugging Face Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JE8CVZ86W2Ru"
      },
      "source": [
        "Using Hugging Face Transformers, you can easily download and run a pre-trained CLIPSeg model on your images. Let’s start by installing transformers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jTjY5YtQi1kT",
        "outputId": "aa736c57-243d-43aa-8f09-a084c419a8b2"
      },
      "outputs": [],
      "source": [
        "# # Check nvcc version\n",
        "# !nvcc -V\n",
        "# # Check GCC version\n",
        "# !gcc --version\n",
        "\n",
        "# # install dependencies: (use cu111 because colab has CUDA 11.1)\n",
        "# %pip install -U openmim\n",
        "# !mim install \"mmengine>=0.7.0\"\n",
        "# !mim install \"mmcv>=2.0.0rc4\"\n",
        "\n",
        "# # Install mmdetection\n",
        "# !rm -rf mmdetection\n",
        "# !git clone https://github.com/open-mmlab/mmdetection.git\n",
        "# %cd mmdetection\n",
        "\n",
        "# %pip install -e .\n",
        "\n",
        "# !pip install torch opencv-python Pillow\n",
        "# !pip install -q 'git+https://github.com/facebookresearch/segment-anything.git'\n",
        "# !pip install -q jupyter_bbox_widget roboflow dataclasses-json supervision\n",
        "# !pip install -q transformers\n",
        "# !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
        "\n",
        "# !pip uninstall -y supervision\n",
        "# !pip install -q supervision==0.6.0\n",
        "!pip install -q jupyter_bbox_widget roboflow dataclasses-json supervision\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2MF3RpSxaw-",
        "outputId": "9b92267d-d835-4ef7-e38d-c62f3f0b2e12"
      },
      "outputs": [],
      "source": [
        "# import sys\n",
        "# sys.path.append('/content')\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# import supervision as sv\n",
        "# print(sv.__version__)\n",
        "import torch\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2jHdaHJ0io7",
        "outputId": "01ec0729-5e06-4395-98b2-f5e02b4d5f8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "processing glip_atss_swin-t_fpn_dyhead_pretrain_obj365-goldg-cc3m-sub...\n",
            "\u001b[32mglip_tiny_mmdet-c24ce662.pth exists in /home/hongxin/weights/glip_tiny_mmdet-c24ce662.pth\u001b[0m\n",
            "\u001b[32mSuccessfully dumped glip_atss_swin-t_fpn_dyhead_pretrain_obj365-goldg-cc3m-sub.py to /home/hongxin/weights/glip_tiny_mmdet-c24ce662.pth\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#GLIP TINY\n",
        "import os\n",
        "\n",
        "HOME = \"/home/hongxin/\"\n",
        "model_name =  'glip_atss_swin-t_fpn_dyhead_pretrain_obj365-goldg-cc3m-sub'\n",
        "GLIP_CHECKPOINT_PATH = os.path.join(HOME, \"weights\", \"glip_tiny_mmdet-c24ce662.pth\")\n",
        "!mim download mmdet --config {model_name} --dest {GLIP_CHECKPOINT_PATH}\n",
        "checkpoint = GLIP_CHECKPOINT_PATH\n",
        "#GLIP LARGE\n",
        "# model_name = 'glip_atss_swin-l_fpn_dyhead_pretrain_mixeddata'\n",
        "# checkpoint =  '/content/checkpoints/glip_l_mmdet-abfe026b.pth'\n",
        "# !mkdir /content/checkpoints\n",
        "# !mim download mmdet --config {modelName} --dest /content/checkpoints\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "BKgPlbq5jUb3",
        "outputId": "aac8234b-31ad-4fbd-bfe1-0c2c9b4f5b4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loads checkpoint by local backend from path: /home/hongxin/weights/glip_tiny_mmdet-c24ce662.pth\n",
            "The model and loaded state dict do not match exactly\n",
            "\n",
            "unexpected key in source state_dict: language_model.language_backbone.body.model.embeddings.position_ids\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/hongxin/mmdetection/mmdet/apis/det_inferencer.py:130: UserWarning: dataset_meta or class names are not saved in the checkpoint's meta data, use COCO classes by default.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12/05 02:24:32 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Failed to search registry with scope \"mmdet\" in the \"function\" registry tree. As a workaround, the current \"function\" registry in \"mmengine\" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether \"mmdet\" is a correct scope, or whether the registry is initialized.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/hongxin/anaconda3/envs/CSCI567/lib/python3.11/site-packages/mmengine/visualization/visualizer.py:196: UserWarning: Failed to add <class 'mmengine.visualization.vis_backend.LocalVisBackend'>, please provide the `save_dir` argument.\n",
            "  warnings.warn(f'Failed to add {vis_backend.__class__}, '\n"
          ]
        }
      ],
      "source": [
        "from mmdet.apis import DetInferencer\n",
        "\n",
        "# Set the device to be used for evaluation\n",
        "device = 'cuda:0'\n",
        "# import sys\n",
        "# sys.path.append(os.path.join(HOME, \"weights\"))\n",
        "# Initialize the DetInferencer\n",
        "inferencer = DetInferencer(model_name, checkpoint, device)\n",
        "\n",
        "\n",
        "# #TEST\n",
        "# # Use the detector to do inference\n",
        "# img = '/content/drive/MyDrive/CSCI 567 /segment-anything/datasets/people_poses/val_images/100034_483681.jpg'\n",
        "# result = inferencer(img, texts='Socks', out_dir='./output')\n",
        "# predictions = result['predictions']\n",
        "\n",
        "# bbox = []\n",
        "# if len(predictions) > 0:\n",
        "#   bboxVal = predictions[0]['bboxes']\n",
        "#   xMin = 0.0\n",
        "#   yMin = 0.0\n",
        "#   xMax = 0.0\n",
        "#   yMax = 0.0\n",
        "#   if len(bboxVal) > 0:\n",
        "#     xMin, yMin, xMax, yMax = bboxVal[0]\n",
        "#   i = 0\n",
        "#   for box in bboxVal:\n",
        "#     if predictions[0]['scores'][i] > 0.5:\n",
        "#       print(box)\n",
        "#       x1, y1, x2, y2 = box\n",
        "#       if x1 < xMin:\n",
        "#         xMin = x1\n",
        "#       if y1 < yMin:\n",
        "#         yMin = y1\n",
        "#       if x2 > xMax:\n",
        "#         xMax = x2\n",
        "#       if y2 > yMax:\n",
        "#         yMax = y2\n",
        "#     i += 1\n",
        "\n",
        "#   bbox = [xMin, yMin, xMax, yMax]\n",
        "\n",
        "\n",
        "# print(\"final\")\n",
        "# print(bbox)\n",
        "\n",
        "# # Show the output image\n",
        "# from PIL import Image\n",
        "# Image.open('./output/vis/100034_483681.jpg')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjnCAKnaU04t"
      },
      "source": [
        "## Text prompting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qc4IuietEqr1"
      },
      "source": [
        "Let’s start by defining some text categories we want to segment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVWkP-j0W8WO"
      },
      "source": [
        "Now that we have our inputs, we can process them and input them to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XfCjKfgtW1_M"
      },
      "outputs": [],
      "source": [
        "def getPredictions(promptValue, image, imagePath):\n",
        "  result = inferencer(imagePath, texts= promptValue)\n",
        "  predictions = result['predictions']\n",
        "\n",
        "  bbox = []\n",
        "  if len(predictions) > 0:\n",
        "    bboxVal = predictions[0]['bboxes']\n",
        "    print(predictions[0]['scores'])\n",
        "    xMin = 0.0\n",
        "    yMin = 0.0\n",
        "    xMax = 0.0\n",
        "    yMax = 0.0\n",
        "    if len(bboxVal) > 0:\n",
        "      xMin, yMin, xMax, yMax = bboxVal[0]\n",
        "    i = 0\n",
        "    for box in bboxVal:\n",
        "      if predictions[0]['scores'][i] > 0.5:\n",
        "        x1, y1, x2, y2 = box\n",
        "        if x1 < xMin:\n",
        "          xMin = x1\n",
        "        if y1 < yMin:\n",
        "          yMin = y1\n",
        "        if x2 > xMax:\n",
        "          xMax = x2\n",
        "        if y2 > yMax:\n",
        "          yMax = y2\n",
        "      i += 1\n",
        "    bbox = [xMin, yMin, xMax, yMax]\n",
        "\n",
        "\n",
        "  if len(bbox) > 0:\n",
        "    return bbox\n",
        "  return [0, 0, 0, 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kphdXZGrc38y"
      },
      "source": [
        "# END OF CHANGES\n",
        "NO CODE BELOW THIS SECTION WAS CHANGED\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gnkj-5DcTLlh"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "def make_square_by_padding(img, fill_color=(0, 0, 0)):\n",
        "    # img = Image.open(requests.get(url, stream=True).raw)\n",
        "    width, height = img.size\n",
        "\n",
        "    # Determine the size for the square\n",
        "    new_size = max(width, height)\n",
        "\n",
        "    # Create a new image with the desired size and fill color\n",
        "    new_img = Image.new(\"RGB\", (new_size, new_size), fill_color)\n",
        "\n",
        "    # Paste the original image onto the center of the new image\n",
        "    new_img.paste(img, ((new_size - width) // 2, (new_size - height) // 2))\n",
        "\n",
        "    return new_img\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5Kp0mOZ7TLlh"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.ops import masks_to_boxes\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image, Image, ImageDraw\n",
        "import numpy as np\n",
        "\n",
        "def processPredictionImage(pred, img):\n",
        "    processed_tensor = torch.sigmoid(torch.reshape(torch.tensor(pred), (352, 352)))\n",
        "    image_np = (processed_tensor.detach().numpy() * 255).astype(np.uint8)\n",
        "\n",
        "    _, binary_image = cv2.threshold(image_np, 127, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    desired_width, desired_height = img.size\n",
        "    resized_image = cv2.resize(binary_image, (max(desired_width, desired_height),  max(desired_width, desired_height)), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    # Get dimensions of the binary image\n",
        "    height, width = resized_image.shape[:2]\n",
        "\n",
        "    # Check if the desired crop size is smaller than the original image size\n",
        "    if desired_width <= width and desired_height <= height:\n",
        "        # Calculate the top-left corner of the crop\n",
        "        x = width // 2 - desired_width // 2\n",
        "        y = height // 2 - desired_height // 2\n",
        "\n",
        "        # Crop the image\n",
        "        cropped_image = resized_image[y:y+desired_height, x:x+desired_width]\n",
        "        return cropped_image\n",
        "\n",
        "    else:\n",
        "        print(\"failed to crop image\")\n",
        "        return resized_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "yhqQnM6NTLlh"
      },
      "outputs": [],
      "source": [
        "\n",
        "def getBoundingBox(predictionImage):\n",
        "    contours, _ = cv2.findContours(predictionImage, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    if contours:\n",
        "        list_of_pts = []\n",
        "        for ctr in contours:\n",
        "            list_of_pts += [pt[0] for pt in ctr]\n",
        "        ctr = np.array(list_of_pts).reshape((-1,1,2)).astype(np.int32)\n",
        "        # largest_contour = max(cv2.convexHull(ctr), key=cv2.contourArea)\n",
        "        x, y, w, h = cv2.boundingRect(cv2.convexHull(ctr))\n",
        "\n",
        "        print(f\"Bounding Box: x={x}, y={y}, width={(w)}, height={(h)}\")\n",
        "        return [x, y, (w + x), (h + y)]\n",
        "\n",
        "    else:\n",
        "        print(\"No contours found\")\n",
        "        return [0, 0, 0, 0]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S5bxvB6TLlh"
      },
      "source": [
        "# SAM Integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWvmCRbSTLlh",
        "outputId": "cf4b1ffa-dba8-4998-9b12-d4f7fa8111e7"
      },
      "outputs": [],
      "source": [
        "!pip install torch opencv-python Pillow\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install -q 'git+https://github.com/facebookresearch/segment-anything.git'\n",
        "!pip install -q jupyter_bbox_widget roboflow dataclasses-json supervision\n",
        "\n",
        "!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Gr-mAlIFTLlh"
      },
      "outputs": [],
      "source": [
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "\n",
        "sam_checkpoint = \"../../sam_vit_h_4b8939.pth\"\n",
        "model_type = \"vit_h\"\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "sam.to(device=device)\n",
        "\n",
        "predictor = SamPredictor(sam)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "R8O64h_ITLlh"
      },
      "outputs": [],
      "source": [
        "def getSAMPreditction(image, box):\n",
        "\n",
        "    image_np = np.array(image)\n",
        "    image2 = cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB)\n",
        "    predictor.set_image(image2)\n",
        "\n",
        "    input_box = np.array(box)\n",
        "    if(input_box[0] == 0 and input_box[1] == 0 and input_box[2] == 0 and input_box[3] == 0):\n",
        "        masks, _, _ = predictor.predict(\n",
        "            point_coords=None,\n",
        "            point_labels=None,\n",
        "            box=None,\n",
        "            multimask_output=False,\n",
        "        )\n",
        "    else:\n",
        "        masks, _, _ = predictor.predict(\n",
        "            point_coords=None,\n",
        "            point_labels=None,\n",
        "            box=input_box[None, :],\n",
        "            multimask_output=False,\n",
        "        )\n",
        "    return masks[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "oBOshUh-TLlh"
      },
      "outputs": [],
      "source": [
        "\n",
        "# numpy version\n",
        "def pixAcc(predicted, target):\n",
        "    same = (predicted == target).sum()\n",
        "    w, h = target.shape\n",
        "    print(\"Target seg shape: {}, Predicted seg shape: {}, #Same pixels: {}\".format(target.shape, predicted.shape, same))\n",
        "    return same / (w * h)\n",
        "\n",
        "# input: bool matrix\n",
        "def IOU(predicted , target):\n",
        "    intersection = np.logical_and(target, predicted).sum()\n",
        "    union = np.logical_or(target, predicted).sum()\n",
        "    if union == 0:\n",
        "        iou_score = 0\n",
        "    else :\n",
        "        iou_score = intersection / union\n",
        "    return iou_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "VhGmJd-KTLlh"
      },
      "outputs": [],
      "source": [
        "\n",
        "def gt_to_anns_of_label_mask(mask_gt):\n",
        "  labels = np.unique(mask_gt)\n",
        "  anns = []\n",
        "  for label in labels:\n",
        "    # skip background\n",
        "      if label == 0:\n",
        "          continue\n",
        "      mask = np.all(mask_gt == label, axis=-1)\n",
        "      anns.append({\n",
        "        'area': np.sum(mask),\n",
        "        'segmentation': mask,\n",
        "        'label': label,\n",
        "      })\n",
        "  return anns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QhkeQOnTLlh"
      },
      "source": [
        "# Final Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "TvSEdqao6WcA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def getFinalPredictions(images, imagePaths, promptVals):\n",
        "  preds = []\n",
        "  boundingBoxes = []\n",
        "  masks = []\n",
        "  emptyPrompt = \"\"\n",
        "  i = 0\n",
        "  for img in images:\n",
        "    imgResults = {}\n",
        "    imageBoundingBoxes = {}\n",
        "    imageMasks = {}\n",
        "    img_with_border = make_square_by_padding(img)\n",
        "\n",
        "    for prompt in promptVals:\n",
        "      # prediction = getPredictions(prompt, img)\n",
        "  #     imgResults[prompt] = prediction\n",
        "  #     predictionImage = processPredictionImage(prediction, img)\n",
        "      print(prompt)\n",
        "      bbox = getPredictions(prompt, img, imagePaths[i])\n",
        "\n",
        "      imageBoundingBoxes[prompt] = bbox\n",
        "      print(bbox)\n",
        "      if(bbox[0] == 0 and bbox[1] == 0 and bbox[2] == 0 and bbox[3] == 0 and emptyPrompt == \"\"):\n",
        "        print(\"Here\")\n",
        "        mask = getSAMPreditction(img, bbox)\n",
        "        imageMasks[prompt] = mask\n",
        "        emptyPrompt = prompt\n",
        "      elif(bbox[0] == 0 and bbox[1] == 0 and bbox[2] == 0 and bbox[3] == 0):\n",
        "        print(\"Here\")\n",
        "        imageMasks[prompt] = imageMasks[emptyPrompt]\n",
        "      else:\n",
        "        mask = getSAMPreditction(img, bbox)\n",
        "        imageMasks[prompt] = mask\n",
        "\n",
        "    emptyPrompt = \"\"\n",
        "    preds.append(imgResults)\n",
        "    boundingBoxes.append(imageBoundingBoxes)\n",
        "    masks.append(imageMasks)\n",
        "    i += 1\n",
        "  preds = np.array(preds)\n",
        "  boundingBoxes = np.array(boundingBoxes)\n",
        "  return boundingBoxes, preds, masks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGwQ0TTvTLli"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "\n",
        "def getImages(num):\n",
        "    images = []\n",
        "    imagePaths = []\n",
        "    truthMasks = []\n",
        "    data_list = []\n",
        "    root = \"\"\n",
        "    root = \"/content/drive/MyDrive/CSCI 567 /segment-anything/datasets/people_poses/\"\n",
        "    textFile = \"val_id.short.txt\"\n",
        "    imageFile = \"val_images/\"\n",
        "    segmentationFile = \"val_segmentations/\"\n",
        "    with open(os.path.join(root, f\"val_id.short.txt\"), 'r') as lf:\n",
        "        data_list = [ s.strip() for s in lf.readlines() ]\n",
        "\n",
        "    num_valid_case, sum_miou, sum_pixAcc = 0,0,0\n",
        "    try:\n",
        "        for data_name in (pbar := tqdm(data_list[:num])):\n",
        "            img_path = root + imageFile + data_name + '.jpg'\n",
        "            seg_path = root +  segmentationFile+ data_name + '.png'\n",
        "\n",
        "            # Read Image and Ground truth mask\n",
        "            img = copy.deepcopy(Image.open(img_path))\n",
        "            imagePaths.append(img_path)\n",
        "            # display(img)\n",
        "            if img is None:\n",
        "                print(\"\\nimage is None\", data_name)\n",
        "                continue\n",
        "            else:\n",
        "                images.append(img)\n",
        "            mask_gt = cv2.imread(seg_path)\n",
        "            if mask_gt is None:\n",
        "                print(\"\\nmask_gt is None\", data_name)\n",
        "                continue\n",
        "            else:\n",
        "                truthMasks.append(mask_gt)\n",
        "    except Exception as e:\n",
        "        print(\"ERROR\")\n",
        "        print(e)\n",
        "    return images, truthMasks, imagePaths\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMf9gQGQTLli"
      },
      "outputs": [],
      "source": [
        "def get_masked_image(original_image, segmentation):\n",
        "  # Visualize\n",
        "\n",
        "  overlay_image = Image.new('RGBA', original_image.size, (0, 0, 0, 0))\n",
        "  overlay_color = (255, 0, 0, 200)\n",
        "\n",
        "  draw = ImageDraw.Draw(overlay_image)\n",
        "  segmentation_mask_image = Image.fromarray(segmentation.astype('uint8') * 255)\n",
        "  draw.bitmap((0, 0), segmentation_mask_image, fill=overlay_color)\n",
        "\n",
        "  return Image.alpha_composite(original_image.convert('RGBA'), overlay_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxSyAahUTLli"
      },
      "outputs": [],
      "source": [
        "\n",
        "def evaluate(images, masks, truthMasks):\n",
        "  # Get confidence scores for each masks generated by SAM for\n",
        "  # each object label existing in the given image\n",
        "\n",
        "  results = []\n",
        "  label_name = [\"Background\",\"Hat\",\"Hair\",\"Glove\",\n",
        "        \"Sunglasses\",\"UpperClothes\",\"Dress\",\"Coat\",\"Socks\",\"Pants\",\n",
        "        \"Jumpsuits\",\"Scarf\",\"Skirt\",\"Face\",\"Left-arm\",\"Right-arm\",\"Left-leg\",\"Right-leg\",\"Left-shoe\",\"Right-shoe\"]\n",
        "\n",
        "\n",
        "  for i in range(len(truthMasks)):\n",
        "    anns = gt_to_anns_of_label_mask(truthMasks[i])\n",
        "    for ann in anns:\n",
        "      mask = masks[i]\n",
        "      image = images[i]\n",
        "      label =  label_name[ann['label']]\n",
        "      print(label_name[ann['label']])\n",
        "      # Get Corresponding gt Mask, generated Mask for Evaluation\n",
        "      print(mask[label])\n",
        "      iou = IOU(mask[label],ann['segmentation'])\n",
        "      print(iou)\n",
        "      result_image = get_masked_image(image, mask[label])\n",
        "      # display(result_image)\n",
        "      pixacc = pixAcc(mask[label],ann['segmentation'])\n",
        "      result = {\n",
        "          \"iou\": iou,\n",
        "          \"pixacc\": pixacc,\n",
        "          \"label_num\": ann[\"label\"],\n",
        "          \"label_name\": label,\n",
        "          \"masked_img\": result_image,\n",
        "          \"pred_mask\": mask[label],\n",
        "          \"gt_mask\": ann\n",
        "      }\n",
        "      print(\"iou:{}, pixacc:{}, label num:{}, label_name:{}\".format(iou, pixacc, ann[\"label\"], label))\n",
        "      results.append(result)\n",
        "\n",
        "  return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pw8pfG8RajZK"
      },
      "source": [
        "# MAIN TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "MFLt-puoTLli",
        "outputId": "0ded4861-b009-420c-a3a4-08d12ef8ffae"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "images, truthMasks, imagePaths = getImages(32)\n",
        "\n",
        "bodyPrompts = [\"Background\",\"Hat\",\"Hair\",\"Glove\", \"Sunglasses\",\"UpperClothes\",\"Dress\",\"Coat\",\"Socks\",\"Pants\", \"Jumpsuits\",\"Scarf\",\"Skirt\",\"Face\",\"Left-arm\",\"Right-arm\",\"Left-leg\",\"Right-leg\",\"Left-shoe\",\"Right-shoe\"]\n",
        "# bodyPrompts = [\"Socks\", \"Hat\"]\n",
        "\n",
        "boundingBoxes, preds, masks = getFinalPredictions(images, imagePaths, bodyPrompts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhC3Sk6hgbth"
      },
      "source": [
        "# SAVING DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HJ7bVm3TLli"
      },
      "outputs": [],
      "source": [
        "def compute_pix_acc(predicted, target):\n",
        "\n",
        "    assert predicted.shape == target.shape\n",
        "    assert len(predicted.shape) == 2\n",
        "    return (predicted == target).mean()\n",
        "\n",
        "def compute_IOU(predicted, target):\n",
        "\n",
        "    assert predicted.shape == target.shape\n",
        "\n",
        "    assert len(predicted.shape) == 2\n",
        "    intersection = np.logical_and(target, predicted).sum()\n",
        "    union = np.logical_or(target, predicted).sum()\n",
        "    assert union > 0\n",
        "    return intersection / union\n",
        "def compute_metric(name, masks, label):\n",
        "    \"\"\" name: data_id\n",
        "        mask: { label_id: numpy.ndarray(shape=(H, W)) }\n",
        "        label: np.ndarray(shape=(H, W)) --> numbers from 0 to 19\n",
        "    \"\"\"\n",
        "    pix_acc_metric = { \"name\": name }\n",
        "    iou_metric = { \"name\": name }\n",
        "    empty = np.zeros_like(label)\n",
        "    LABELS = [\"Background\",\"Hat\",\"Hair\",\"Glove\",\n",
        "        \"Sunglasses\",\"UpperClothes\",\"Dress\",\"Coat\",\"Socks\",\"Pants\",\n",
        "        \"Jumpsuits\",\"Scarf\",\"Skirt\",\"Face\",\"Left-arm\",\"Right-arm\",\"Left-leg\",\"Right-leg\",\"Left-shoe\",\"Right-shoe\"]\n",
        "\n",
        "    for i, label_name in enumerate(LABELS):\n",
        "        mask_i = masks.get(label_name, empty)\n",
        "        label_i = (label == i)\n",
        "        if label_i.sum() == 0:\n",
        "            # pandas dataframe automatically skips nan\n",
        "            # when computing .count() and .mean()\n",
        "            iou_metric[label_name] = np.nan\n",
        "            pix_acc_metric[label_name] = np.nan\n",
        "        else:\n",
        "            iou_metric[label_name] = compute_IOU(mask_i, label_i)\n",
        "            pix_acc_metric[label_name] = compute_pix_acc(mask_i, label_i)\n",
        "\n",
        "    return iou_metric, pix_acc_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9o_V-NyTLli"
      },
      "outputs": [],
      "source": [
        "def getImages2(num):\n",
        "    images = []\n",
        "    truthMasks = []\n",
        "    data_list2 = []\n",
        "    root = \"\"\n",
        "    # root = \"/content/drive/MyDrive/CSCI567/segment-anything/datasets/people_poses/\"\n",
        "    textFile = \"val_id.short.txt\"\n",
        "    imageFile = \"val_images/\"\n",
        "    segmentationFile = \"val_segmentations/\"\n",
        "    with open(os.path.join(root, f\"val_id.short.txt\"), 'r') as lf:\n",
        "        data_list = [ s.strip() for s in lf.readlines() ]\n",
        "\n",
        "    num_valid_case, sum_miou, sum_pixAcc = 0,0,0\n",
        "    try:\n",
        "        for data_name in (pbar := tqdm(data_list[:num])):\n",
        "            img_path = root + imageFile + data_name + '.jpg'\n",
        "            seg_path = root +  segmentationFile+ data_name + '.png'\n",
        "\n",
        "            # Read Image and Ground truth mask\n",
        "            img = copy.deepcopy(Image.open(img_path))\n",
        "\n",
        "            # display(img)\n",
        "            if img is None:\n",
        "                print(\"\\nimage is None\", data_name)\n",
        "                continue\n",
        "            else:\n",
        "                data_list2.append(data_name)\n",
        "                images.append(img)\n",
        "            mask_gt = cv2.imread(seg_path)\n",
        "            if mask_gt is None:\n",
        "                print(\"\\nmask_gt is None\", data_name)\n",
        "                continue\n",
        "            else:\n",
        "                truthMasks.append(mask_gt)\n",
        "    except Exception as e:\n",
        "        print(\"ERROR\")\n",
        "        print(e)\n",
        "    return images, truthMasks, data_list2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwhRO0JRTLli"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# results = evaluate(images, masks, truthMasks)\n",
        "from PIL import Image, Image, ImageDraw\n",
        "from torch.jit import Error\n",
        "import pandas as pd\n",
        "# numpy metrics\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "# root = \"/content/drive/MyDrive/CSCI567/segment-anything/datasets/people_poses/\"\n",
        "root = \"\"\n",
        "prompt = \"The object of \"\n",
        "\n",
        "# try:\n",
        "miou_table = []\n",
        "pix_acc_table = []\n",
        "images, truthMasks, data_list = getImages2(32)\n",
        "print(len(truthMasks))\n",
        "print(len(images))\n",
        "print(len(data_list))\n",
        "\n",
        "result_table = {}\n",
        "i = 0\n",
        "try:\n",
        "  for data_name in data_list:\n",
        "\n",
        "    anns = gt_to_anns_of_label_mask(truthMasks[i])\n",
        "    miou, pix_acc = compute_metric(data_name, masks[i], truthMasks[i][:,:,0])\n",
        "    miou_table.append(miou)\n",
        "    pix_acc_table.append(pix_acc)\n",
        "    predict_anns = []\n",
        "    for ann in anns:\n",
        "\n",
        "      ## Get Label Index with Highest Score\n",
        "      predict_anns.append({\n",
        "          'segmentation': masks[i],\n",
        "          'label': ann['label'],\n",
        "          'gt': ann['segmentation'],\n",
        "      })\n",
        "    i += 1\n",
        "    result_table[data_name] = predict_anns\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "  print(miou_table)\n",
        "  print(pix_acc_table)\n",
        "\n",
        "miou_table = pd.DataFrame(miou_table, columns=miou_table[0].keys()).set_index('name')\n",
        "miou_table.to_csv('/Users/arianasokolov/Desktop/ClipSegSam/CSVs/miou.csv')\n",
        "np.save('vis_cliseg_sam_32.npy', result_table)\n",
        "\n",
        "pix_acc_table = pd.DataFrame(pix_acc_table, columns=pix_acc_table[0].keys()).set_index('name')\n",
        "pix_acc_table.to_csv('/Users/arianasokolov/Desktop/ClipSegSam/CSVs/pix_acc.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rep8ON9ggfk8"
      },
      "source": [
        "# VISUALIZATION\n",
        "\n",
        "EVERYTHING BELOW THIS SECTION IS JUST TO TEST THINGS AND DOESN'T MATTER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eemhzXM4TLli"
      },
      "outputs": [],
      "source": [
        "print(result_table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "os0-rQ2KTLli"
      },
      "outputs": [],
      "source": [
        "def show_mask(mask, ax, random_color=False):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)\n",
        "def get_mask(mask,color=None):\n",
        "    if color is None:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    # else:\n",
        "    #     color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    return mask_image\n",
        "\n",
        "def show_box(box, ax):\n",
        "    x0, y0 = box[0], box[1]\n",
        "    w, h = box[2] - box[0], box[3] - box[1]\n",
        "    return plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ug7mAq6uTLli"
      },
      "outputs": [],
      "source": [
        "prompts = bodyPrompts[:2]\n",
        "last = 1\n",
        "# *****\n",
        "# Setup plot\n",
        "_, ax = plt.subplots(len(boundingBoxes[:last]), len(prompts) + 1, figsize=(3*(len(prompts) + 1), 4))\n",
        "[a.axis('off') for a in ax.flatten()]\n",
        "\n",
        "for i in range(len(boundingBoxes[:last])):\n",
        "  # Show padded image\n",
        "  print(imagePaths[i])\n",
        "  ax[0].imshow(images[i])\n",
        "\n",
        "  for j in range(len(prompts)):\n",
        "    # Show mask\n",
        "    print(prompts[j])\n",
        "    print(preds[i])\n",
        "    # predictionImage = processPredictionImage(preds[i][prompts[j]], images[i])\n",
        "    # ax[i][j+1].imshow(predictionImage)\n",
        "    # ax[i][j+1].imshow(get_mask(masks[0][\"Socks\"]))\n",
        "\n",
        "    # Show bounding box\n",
        "    print(boundingBoxes[i][prompts[j]])\n",
        "    ax[0].add_patch(show_box(boundingBoxes[i][prompts[j]], plt.gca()))\n",
        "\n",
        "\n",
        "    # Show prompt\n",
        "    # ax[i][j+1].text(0, -15, prompts[j])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcHmNt2-TLli"
      },
      "outputs": [],
      "source": [
        "input_box = np.array(boundingBoxes[0][\"Socks\"])\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(images[0])\n",
        "show_mask(masks[0][\"Socks\"], plt.gca())\n",
        "# plt.gca().add_patch(show_box(input_box, plt.gca()))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-coJ3rY4TLli"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "rhC3Sk6hgbth"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "c1330ff811d49988c6bd3f6d30257d79d8234a7d606d1a91e87e277837a80053"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
